{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (0.10.11)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (0.4.13)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (3.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (1.24.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from jax->mediapipe) (7.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from matplotlib->mediapipe) (6.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda31\\envs\\llm\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Import mediapipe\n",
    "import cv2 # Import opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make Some Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe holistic model and drawing utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Start capturing video from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed to RGB for Mediapipe processing\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks (no FACE_CONNECTIONS, just landmarks)\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, \n",
    "                                      mp.solutions.holistic.FACEMESH_TESSELATION,  # Use FACEMESH_TESSELATION for face mesh connections\n",
    "                                      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # Show the processed image\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Initialize mediapipe holistic model and drawing utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path to input video\n",
    "video_path = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\output_videos\\Vid3.mp4'\n",
    "\n",
    "# Specify output folder and file\n",
    "output_folder = 'output_videos'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_video_path = os.path.join(output_folder, r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\output_videos\\Vid3Processed.mp4')\n",
    "\n",
    "# Start capturing video from the file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get frame width, height, and FPS for saving the output video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed to RGB for Mediapipe processing\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, \n",
    "                                      mp.solutions.holistic.FACEMESH_TESSELATION, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # Write the processed frame to the output video\n",
    "        out.write(image)\n",
    "        \n",
    "        # Show the processed image (optional)\n",
    "        cv2.imshow('Processed Video Feed', image)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and writer objects, and close windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Initialize mediapipe pose model and drawing utilities\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path to input video\n",
    "video_path = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\output_videos\\Vid3.mp4'\n",
    "\n",
    "# Specify output folder and file\n",
    "output_folder = 'output_videos'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_video_path = os.path.join(output_folder, r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\output_videos\\MainProcessed.mp4')\n",
    "\n",
    "# Start capturing video from the file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get frame width, height, and FPS for saving the output video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initiate pose model\n",
    "with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=1) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed to RGB for Mediapipe processing\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Draw Pose landmarks for the detected person\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, \n",
    "                                      mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # Write the processed frame to the output video\n",
    "        out.write(image)\n",
    "        \n",
    "        # Show the processed image (optional)\n",
    "        cv2.imshow('Processed Video Feed', image)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and writer objects, and close windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995158314704895"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.face_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 0.6766873002052307\n",
      "y: 0.38457825779914856\n",
      "z: -0.061855755746364594\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6766873002052307"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results.face_landmarks.landmark[0])\n",
    "results.face_landmarks.landmark[0].x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Capture Landmarks & Export to CSV\n",
    "<!--<img src=\"https://i.imgur.com/8bForKY.png\">-->\n",
    "<!--<img src=\"https://i.imgur.com/AzKNp7A.png\">-->\n",
    "<!--<img src=\"https://i.imgur.com/8bForKY.png\">-->\n",
    "<img src=\"https://i.imgur.com/AzKNp7A.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coords = len(results.pose_landmarks.landmark)+len(results.face_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'x1',\n",
       " 'y1',\n",
       " 'z1',\n",
       " 'v1',\n",
       " 'x2',\n",
       " 'y2',\n",
       " 'z2',\n",
       " 'v2',\n",
       " 'x3',\n",
       " 'y3',\n",
       " 'z3',\n",
       " 'v3',\n",
       " 'x4',\n",
       " 'y4',\n",
       " 'z4',\n",
       " 'v4',\n",
       " 'x5',\n",
       " 'y5',\n",
       " 'z5',\n",
       " 'v5',\n",
       " 'x6',\n",
       " 'y6',\n",
       " 'z6',\n",
       " 'v6',\n",
       " 'x7',\n",
       " 'y7',\n",
       " 'z7',\n",
       " 'v7',\n",
       " 'x8',\n",
       " 'y8',\n",
       " 'z8',\n",
       " 'v8',\n",
       " 'x9',\n",
       " 'y9',\n",
       " 'z9',\n",
       " 'v9',\n",
       " 'x10',\n",
       " 'y10',\n",
       " 'z10',\n",
       " 'v10',\n",
       " 'x11',\n",
       " 'y11',\n",
       " 'z11',\n",
       " 'v11',\n",
       " 'x12',\n",
       " 'y12',\n",
       " 'z12',\n",
       " 'v12',\n",
       " 'x13',\n",
       " 'y13',\n",
       " 'z13',\n",
       " 'v13',\n",
       " 'x14',\n",
       " 'y14',\n",
       " 'z14',\n",
       " 'v14',\n",
       " 'x15',\n",
       " 'y15',\n",
       " 'z15',\n",
       " 'v15',\n",
       " 'x16',\n",
       " 'y16',\n",
       " 'z16',\n",
       " 'v16',\n",
       " 'x17',\n",
       " 'y17',\n",
       " 'z17',\n",
       " 'v17',\n",
       " 'x18',\n",
       " 'y18',\n",
       " 'z18',\n",
       " 'v18',\n",
       " 'x19',\n",
       " 'y19',\n",
       " 'z19',\n",
       " 'v19',\n",
       " 'x20',\n",
       " 'y20',\n",
       " 'z20',\n",
       " 'v20',\n",
       " 'x21',\n",
       " 'y21',\n",
       " 'z21',\n",
       " 'v21',\n",
       " 'x22',\n",
       " 'y22',\n",
       " 'z22',\n",
       " 'v22',\n",
       " 'x23',\n",
       " 'y23',\n",
       " 'z23',\n",
       " 'v23',\n",
       " 'x24',\n",
       " 'y24',\n",
       " 'z24',\n",
       " 'v24',\n",
       " 'x25',\n",
       " 'y25',\n",
       " 'z25',\n",
       " 'v25',\n",
       " 'x26',\n",
       " 'y26',\n",
       " 'z26',\n",
       " 'v26',\n",
       " 'x27',\n",
       " 'y27',\n",
       " 'z27',\n",
       " 'v27',\n",
       " 'x28',\n",
       " 'y28',\n",
       " 'z28',\n",
       " 'v28',\n",
       " 'x29',\n",
       " 'y29',\n",
       " 'z29',\n",
       " 'v29',\n",
       " 'x30',\n",
       " 'y30',\n",
       " 'z30',\n",
       " 'v30',\n",
       " 'x31',\n",
       " 'y31',\n",
       " 'z31',\n",
       " 'v31',\n",
       " 'x32',\n",
       " 'y32',\n",
       " 'z32',\n",
       " 'v32',\n",
       " 'x33',\n",
       " 'y33',\n",
       " 'z33',\n",
       " 'v33',\n",
       " 'x34',\n",
       " 'y34',\n",
       " 'z34',\n",
       " 'v34',\n",
       " 'x35',\n",
       " 'y35',\n",
       " 'z35',\n",
       " 'v35',\n",
       " 'x36',\n",
       " 'y36',\n",
       " 'z36',\n",
       " 'v36',\n",
       " 'x37',\n",
       " 'y37',\n",
       " 'z37',\n",
       " 'v37',\n",
       " 'x38',\n",
       " 'y38',\n",
       " 'z38',\n",
       " 'v38',\n",
       " 'x39',\n",
       " 'y39',\n",
       " 'z39',\n",
       " 'v39',\n",
       " 'x40',\n",
       " 'y40',\n",
       " 'z40',\n",
       " 'v40',\n",
       " 'x41',\n",
       " 'y41',\n",
       " 'z41',\n",
       " 'v41',\n",
       " 'x42',\n",
       " 'y42',\n",
       " 'z42',\n",
       " 'v42',\n",
       " 'x43',\n",
       " 'y43',\n",
       " 'z43',\n",
       " 'v43',\n",
       " 'x44',\n",
       " 'y44',\n",
       " 'z44',\n",
       " 'v44',\n",
       " 'x45',\n",
       " 'y45',\n",
       " 'z45',\n",
       " 'v45',\n",
       " 'x46',\n",
       " 'y46',\n",
       " 'z46',\n",
       " 'v46',\n",
       " 'x47',\n",
       " 'y47',\n",
       " 'z47',\n",
       " 'v47',\n",
       " 'x48',\n",
       " 'y48',\n",
       " 'z48',\n",
       " 'v48',\n",
       " 'x49',\n",
       " 'y49',\n",
       " 'z49',\n",
       " 'v49',\n",
       " 'x50',\n",
       " 'y50',\n",
       " 'z50',\n",
       " 'v50',\n",
       " 'x51',\n",
       " 'y51',\n",
       " 'z51',\n",
       " 'v51',\n",
       " 'x52',\n",
       " 'y52',\n",
       " 'z52',\n",
       " 'v52',\n",
       " 'x53',\n",
       " 'y53',\n",
       " 'z53',\n",
       " 'v53',\n",
       " 'x54',\n",
       " 'y54',\n",
       " 'z54',\n",
       " 'v54',\n",
       " 'x55',\n",
       " 'y55',\n",
       " 'z55',\n",
       " 'v55',\n",
       " 'x56',\n",
       " 'y56',\n",
       " 'z56',\n",
       " 'v56',\n",
       " 'x57',\n",
       " 'y57',\n",
       " 'z57',\n",
       " 'v57',\n",
       " 'x58',\n",
       " 'y58',\n",
       " 'z58',\n",
       " 'v58',\n",
       " 'x59',\n",
       " 'y59',\n",
       " 'z59',\n",
       " 'v59',\n",
       " 'x60',\n",
       " 'y60',\n",
       " 'z60',\n",
       " 'v60',\n",
       " 'x61',\n",
       " 'y61',\n",
       " 'z61',\n",
       " 'v61',\n",
       " 'x62',\n",
       " 'y62',\n",
       " 'z62',\n",
       " 'v62',\n",
       " 'x63',\n",
       " 'y63',\n",
       " 'z63',\n",
       " 'v63',\n",
       " 'x64',\n",
       " 'y64',\n",
       " 'z64',\n",
       " 'v64',\n",
       " 'x65',\n",
       " 'y65',\n",
       " 'z65',\n",
       " 'v65',\n",
       " 'x66',\n",
       " 'y66',\n",
       " 'z66',\n",
       " 'v66',\n",
       " 'x67',\n",
       " 'y67',\n",
       " 'z67',\n",
       " 'v67',\n",
       " 'x68',\n",
       " 'y68',\n",
       " 'z68',\n",
       " 'v68',\n",
       " 'x69',\n",
       " 'y69',\n",
       " 'z69',\n",
       " 'v69',\n",
       " 'x70',\n",
       " 'y70',\n",
       " 'z70',\n",
       " 'v70',\n",
       " 'x71',\n",
       " 'y71',\n",
       " 'z71',\n",
       " 'v71',\n",
       " 'x72',\n",
       " 'y72',\n",
       " 'z72',\n",
       " 'v72',\n",
       " 'x73',\n",
       " 'y73',\n",
       " 'z73',\n",
       " 'v73',\n",
       " 'x74',\n",
       " 'y74',\n",
       " 'z74',\n",
       " 'v74',\n",
       " 'x75',\n",
       " 'y75',\n",
       " 'z75',\n",
       " 'v75',\n",
       " 'x76',\n",
       " 'y76',\n",
       " 'z76',\n",
       " 'v76',\n",
       " 'x77',\n",
       " 'y77',\n",
       " 'z77',\n",
       " 'v77',\n",
       " 'x78',\n",
       " 'y78',\n",
       " 'z78',\n",
       " 'v78',\n",
       " 'x79',\n",
       " 'y79',\n",
       " 'z79',\n",
       " 'v79',\n",
       " 'x80',\n",
       " 'y80',\n",
       " 'z80',\n",
       " 'v80',\n",
       " 'x81',\n",
       " 'y81',\n",
       " 'z81',\n",
       " 'v81',\n",
       " 'x82',\n",
       " 'y82',\n",
       " 'z82',\n",
       " 'v82',\n",
       " 'x83',\n",
       " 'y83',\n",
       " 'z83',\n",
       " 'v83',\n",
       " 'x84',\n",
       " 'y84',\n",
       " 'z84',\n",
       " 'v84',\n",
       " 'x85',\n",
       " 'y85',\n",
       " 'z85',\n",
       " 'v85',\n",
       " 'x86',\n",
       " 'y86',\n",
       " 'z86',\n",
       " 'v86',\n",
       " 'x87',\n",
       " 'y87',\n",
       " 'z87',\n",
       " 'v87',\n",
       " 'x88',\n",
       " 'y88',\n",
       " 'z88',\n",
       " 'v88',\n",
       " 'x89',\n",
       " 'y89',\n",
       " 'z89',\n",
       " 'v89',\n",
       " 'x90',\n",
       " 'y90',\n",
       " 'z90',\n",
       " 'v90',\n",
       " 'x91',\n",
       " 'y91',\n",
       " 'z91',\n",
       " 'v91',\n",
       " 'x92',\n",
       " 'y92',\n",
       " 'z92',\n",
       " 'v92',\n",
       " 'x93',\n",
       " 'y93',\n",
       " 'z93',\n",
       " 'v93',\n",
       " 'x94',\n",
       " 'y94',\n",
       " 'z94',\n",
       " 'v94',\n",
       " 'x95',\n",
       " 'y95',\n",
       " 'z95',\n",
       " 'v95',\n",
       " 'x96',\n",
       " 'y96',\n",
       " 'z96',\n",
       " 'v96',\n",
       " 'x97',\n",
       " 'y97',\n",
       " 'z97',\n",
       " 'v97',\n",
       " 'x98',\n",
       " 'y98',\n",
       " 'z98',\n",
       " 'v98',\n",
       " 'x99',\n",
       " 'y99',\n",
       " 'z99',\n",
       " 'v99',\n",
       " 'x100',\n",
       " 'y100',\n",
       " 'z100',\n",
       " 'v100',\n",
       " 'x101',\n",
       " 'y101',\n",
       " 'z101',\n",
       " 'v101',\n",
       " 'x102',\n",
       " 'y102',\n",
       " 'z102',\n",
       " 'v102',\n",
       " 'x103',\n",
       " 'y103',\n",
       " 'z103',\n",
       " 'v103',\n",
       " 'x104',\n",
       " 'y104',\n",
       " 'z104',\n",
       " 'v104',\n",
       " 'x105',\n",
       " 'y105',\n",
       " 'z105',\n",
       " 'v105',\n",
       " 'x106',\n",
       " 'y106',\n",
       " 'z106',\n",
       " 'v106',\n",
       " 'x107',\n",
       " 'y107',\n",
       " 'z107',\n",
       " 'v107',\n",
       " 'x108',\n",
       " 'y108',\n",
       " 'z108',\n",
       " 'v108',\n",
       " 'x109',\n",
       " 'y109',\n",
       " 'z109',\n",
       " 'v109',\n",
       " 'x110',\n",
       " 'y110',\n",
       " 'z110',\n",
       " 'v110',\n",
       " 'x111',\n",
       " 'y111',\n",
       " 'z111',\n",
       " 'v111',\n",
       " 'x112',\n",
       " 'y112',\n",
       " 'z112',\n",
       " 'v112',\n",
       " 'x113',\n",
       " 'y113',\n",
       " 'z113',\n",
       " 'v113',\n",
       " 'x114',\n",
       " 'y114',\n",
       " 'z114',\n",
       " 'v114',\n",
       " 'x115',\n",
       " 'y115',\n",
       " 'z115',\n",
       " 'v115',\n",
       " 'x116',\n",
       " 'y116',\n",
       " 'z116',\n",
       " 'v116',\n",
       " 'x117',\n",
       " 'y117',\n",
       " 'z117',\n",
       " 'v117',\n",
       " 'x118',\n",
       " 'y118',\n",
       " 'z118',\n",
       " 'v118',\n",
       " 'x119',\n",
       " 'y119',\n",
       " 'z119',\n",
       " 'v119',\n",
       " 'x120',\n",
       " 'y120',\n",
       " 'z120',\n",
       " 'v120',\n",
       " 'x121',\n",
       " 'y121',\n",
       " 'z121',\n",
       " 'v121',\n",
       " 'x122',\n",
       " 'y122',\n",
       " 'z122',\n",
       " 'v122',\n",
       " 'x123',\n",
       " 'y123',\n",
       " 'z123',\n",
       " 'v123',\n",
       " 'x124',\n",
       " 'y124',\n",
       " 'z124',\n",
       " 'v124',\n",
       " 'x125',\n",
       " 'y125',\n",
       " 'z125',\n",
       " 'v125',\n",
       " 'x126',\n",
       " 'y126',\n",
       " 'z126',\n",
       " 'v126',\n",
       " 'x127',\n",
       " 'y127',\n",
       " 'z127',\n",
       " 'v127',\n",
       " 'x128',\n",
       " 'y128',\n",
       " 'z128',\n",
       " 'v128',\n",
       " 'x129',\n",
       " 'y129',\n",
       " 'z129',\n",
       " 'v129',\n",
       " 'x130',\n",
       " 'y130',\n",
       " 'z130',\n",
       " 'v130',\n",
       " 'x131',\n",
       " 'y131',\n",
       " 'z131',\n",
       " 'v131',\n",
       " 'x132',\n",
       " 'y132',\n",
       " 'z132',\n",
       " 'v132',\n",
       " 'x133',\n",
       " 'y133',\n",
       " 'z133',\n",
       " 'v133',\n",
       " 'x134',\n",
       " 'y134',\n",
       " 'z134',\n",
       " 'v134',\n",
       " 'x135',\n",
       " 'y135',\n",
       " 'z135',\n",
       " 'v135',\n",
       " 'x136',\n",
       " 'y136',\n",
       " 'z136',\n",
       " 'v136',\n",
       " 'x137',\n",
       " 'y137',\n",
       " 'z137',\n",
       " 'v137',\n",
       " 'x138',\n",
       " 'y138',\n",
       " 'z138',\n",
       " 'v138',\n",
       " 'x139',\n",
       " 'y139',\n",
       " 'z139',\n",
       " 'v139',\n",
       " 'x140',\n",
       " 'y140',\n",
       " 'z140',\n",
       " 'v140',\n",
       " 'x141',\n",
       " 'y141',\n",
       " 'z141',\n",
       " 'v141',\n",
       " 'x142',\n",
       " 'y142',\n",
       " 'z142',\n",
       " 'v142',\n",
       " 'x143',\n",
       " 'y143',\n",
       " 'z143',\n",
       " 'v143',\n",
       " 'x144',\n",
       " 'y144',\n",
       " 'z144',\n",
       " 'v144',\n",
       " 'x145',\n",
       " 'y145',\n",
       " 'z145',\n",
       " 'v145',\n",
       " 'x146',\n",
       " 'y146',\n",
       " 'z146',\n",
       " 'v146',\n",
       " 'x147',\n",
       " 'y147',\n",
       " 'z147',\n",
       " 'v147',\n",
       " 'x148',\n",
       " 'y148',\n",
       " 'z148',\n",
       " 'v148',\n",
       " 'x149',\n",
       " 'y149',\n",
       " 'z149',\n",
       " 'v149',\n",
       " 'x150',\n",
       " 'y150',\n",
       " 'z150',\n",
       " 'v150',\n",
       " 'x151',\n",
       " 'y151',\n",
       " 'z151',\n",
       " 'v151',\n",
       " 'x152',\n",
       " 'y152',\n",
       " 'z152',\n",
       " 'v152',\n",
       " 'x153',\n",
       " 'y153',\n",
       " 'z153',\n",
       " 'v153',\n",
       " 'x154',\n",
       " 'y154',\n",
       " 'z154',\n",
       " 'v154',\n",
       " 'x155',\n",
       " 'y155',\n",
       " 'z155',\n",
       " 'v155',\n",
       " 'x156',\n",
       " 'y156',\n",
       " 'z156',\n",
       " 'v156',\n",
       " 'x157',\n",
       " 'y157',\n",
       " 'z157',\n",
       " 'v157',\n",
       " 'x158',\n",
       " 'y158',\n",
       " 'z158',\n",
       " 'v158',\n",
       " 'x159',\n",
       " 'y159',\n",
       " 'z159',\n",
       " 'v159',\n",
       " 'x160',\n",
       " 'y160',\n",
       " 'z160',\n",
       " 'v160',\n",
       " 'x161',\n",
       " 'y161',\n",
       " 'z161',\n",
       " 'v161',\n",
       " 'x162',\n",
       " 'y162',\n",
       " 'z162',\n",
       " 'v162',\n",
       " 'x163',\n",
       " 'y163',\n",
       " 'z163',\n",
       " 'v163',\n",
       " 'x164',\n",
       " 'y164',\n",
       " 'z164',\n",
       " 'v164',\n",
       " 'x165',\n",
       " 'y165',\n",
       " 'z165',\n",
       " 'v165',\n",
       " 'x166',\n",
       " 'y166',\n",
       " 'z166',\n",
       " 'v166',\n",
       " 'x167',\n",
       " 'y167',\n",
       " 'z167',\n",
       " 'v167',\n",
       " 'x168',\n",
       " 'y168',\n",
       " 'z168',\n",
       " 'v168',\n",
       " 'x169',\n",
       " 'y169',\n",
       " 'z169',\n",
       " 'v169',\n",
       " 'x170',\n",
       " 'y170',\n",
       " 'z170',\n",
       " 'v170',\n",
       " 'x171',\n",
       " 'y171',\n",
       " 'z171',\n",
       " 'v171',\n",
       " 'x172',\n",
       " 'y172',\n",
       " 'z172',\n",
       " 'v172',\n",
       " 'x173',\n",
       " 'y173',\n",
       " 'z173',\n",
       " 'v173',\n",
       " 'x174',\n",
       " 'y174',\n",
       " 'z174',\n",
       " 'v174',\n",
       " 'x175',\n",
       " 'y175',\n",
       " 'z175',\n",
       " 'v175',\n",
       " 'x176',\n",
       " 'y176',\n",
       " 'z176',\n",
       " 'v176',\n",
       " 'x177',\n",
       " 'y177',\n",
       " 'z177',\n",
       " 'v177',\n",
       " 'x178',\n",
       " 'y178',\n",
       " 'z178',\n",
       " 'v178',\n",
       " 'x179',\n",
       " 'y179',\n",
       " 'z179',\n",
       " 'v179',\n",
       " 'x180',\n",
       " 'y180',\n",
       " 'z180',\n",
       " 'v180',\n",
       " 'x181',\n",
       " 'y181',\n",
       " 'z181',\n",
       " 'v181',\n",
       " 'x182',\n",
       " 'y182',\n",
       " 'z182',\n",
       " 'v182',\n",
       " 'x183',\n",
       " 'y183',\n",
       " 'z183',\n",
       " 'v183',\n",
       " 'x184',\n",
       " 'y184',\n",
       " 'z184',\n",
       " 'v184',\n",
       " 'x185',\n",
       " 'y185',\n",
       " 'z185',\n",
       " 'v185',\n",
       " 'x186',\n",
       " 'y186',\n",
       " 'z186',\n",
       " 'v186',\n",
       " 'x187',\n",
       " 'y187',\n",
       " 'z187',\n",
       " 'v187',\n",
       " 'x188',\n",
       " 'y188',\n",
       " 'z188',\n",
       " 'v188',\n",
       " 'x189',\n",
       " 'y189',\n",
       " 'z189',\n",
       " 'v189',\n",
       " 'x190',\n",
       " 'y190',\n",
       " 'z190',\n",
       " 'v190',\n",
       " 'x191',\n",
       " 'y191',\n",
       " 'z191',\n",
       " 'v191',\n",
       " 'x192',\n",
       " 'y192',\n",
       " 'z192',\n",
       " 'v192',\n",
       " 'x193',\n",
       " 'y193',\n",
       " 'z193',\n",
       " 'v193',\n",
       " 'x194',\n",
       " 'y194',\n",
       " 'z194',\n",
       " 'v194',\n",
       " 'x195',\n",
       " 'y195',\n",
       " 'z195',\n",
       " 'v195',\n",
       " 'x196',\n",
       " 'y196',\n",
       " 'z196',\n",
       " 'v196',\n",
       " 'x197',\n",
       " 'y197',\n",
       " 'z197',\n",
       " 'v197',\n",
       " 'x198',\n",
       " 'y198',\n",
       " 'z198',\n",
       " 'v198',\n",
       " 'x199',\n",
       " 'y199',\n",
       " 'z199',\n",
       " 'v199',\n",
       " 'x200',\n",
       " 'y200',\n",
       " 'z200',\n",
       " 'v200',\n",
       " 'x201',\n",
       " 'y201',\n",
       " 'z201',\n",
       " 'v201',\n",
       " 'x202',\n",
       " 'y202',\n",
       " 'z202',\n",
       " 'v202',\n",
       " 'x203',\n",
       " 'y203',\n",
       " 'z203',\n",
       " 'v203',\n",
       " 'x204',\n",
       " 'y204',\n",
       " 'z204',\n",
       " 'v204',\n",
       " 'x205',\n",
       " 'y205',\n",
       " 'z205',\n",
       " 'v205',\n",
       " 'x206',\n",
       " 'y206',\n",
       " 'z206',\n",
       " 'v206',\n",
       " 'x207',\n",
       " 'y207',\n",
       " 'z207',\n",
       " 'v207',\n",
       " 'x208',\n",
       " 'y208',\n",
       " 'z208',\n",
       " 'v208',\n",
       " 'x209',\n",
       " 'y209',\n",
       " 'z209',\n",
       " 'v209',\n",
       " 'x210',\n",
       " 'y210',\n",
       " 'z210',\n",
       " 'v210',\n",
       " 'x211',\n",
       " 'y211',\n",
       " 'z211',\n",
       " 'v211',\n",
       " 'x212',\n",
       " 'y212',\n",
       " 'z212',\n",
       " 'v212',\n",
       " 'x213',\n",
       " 'y213',\n",
       " 'z213',\n",
       " 'v213',\n",
       " 'x214',\n",
       " 'y214',\n",
       " 'z214',\n",
       " 'v214',\n",
       " 'x215',\n",
       " 'y215',\n",
       " 'z215',\n",
       " 'v215',\n",
       " 'x216',\n",
       " 'y216',\n",
       " 'z216',\n",
       " 'v216',\n",
       " 'x217',\n",
       " 'y217',\n",
       " 'z217',\n",
       " 'v217',\n",
       " 'x218',\n",
       " 'y218',\n",
       " 'z218',\n",
       " 'v218',\n",
       " 'x219',\n",
       " 'y219',\n",
       " 'z219',\n",
       " 'v219',\n",
       " 'x220',\n",
       " 'y220',\n",
       " 'z220',\n",
       " 'v220',\n",
       " 'x221',\n",
       " 'y221',\n",
       " 'z221',\n",
       " 'v221',\n",
       " 'x222',\n",
       " 'y222',\n",
       " 'z222',\n",
       " 'v222',\n",
       " 'x223',\n",
       " 'y223',\n",
       " 'z223',\n",
       " 'v223',\n",
       " 'x224',\n",
       " 'y224',\n",
       " 'z224',\n",
       " 'v224',\n",
       " 'x225',\n",
       " 'y225',\n",
       " 'z225',\n",
       " 'v225',\n",
       " 'x226',\n",
       " 'y226',\n",
       " 'z226',\n",
       " 'v226',\n",
       " 'x227',\n",
       " 'y227',\n",
       " 'z227',\n",
       " 'v227',\n",
       " 'x228',\n",
       " 'y228',\n",
       " 'z228',\n",
       " 'v228',\n",
       " 'x229',\n",
       " 'y229',\n",
       " 'z229',\n",
       " 'v229',\n",
       " 'x230',\n",
       " 'y230',\n",
       " 'z230',\n",
       " 'v230',\n",
       " 'x231',\n",
       " 'y231',\n",
       " 'z231',\n",
       " 'v231',\n",
       " 'x232',\n",
       " 'y232',\n",
       " 'z232',\n",
       " 'v232',\n",
       " 'x233',\n",
       " 'y233',\n",
       " 'z233',\n",
       " 'v233',\n",
       " 'x234',\n",
       " 'y234',\n",
       " 'z234',\n",
       " 'v234',\n",
       " 'x235',\n",
       " 'y235',\n",
       " 'z235',\n",
       " 'v235',\n",
       " 'x236',\n",
       " 'y236',\n",
       " 'z236',\n",
       " 'v236',\n",
       " 'x237',\n",
       " 'y237',\n",
       " 'z237',\n",
       " 'v237',\n",
       " 'x238',\n",
       " 'y238',\n",
       " 'z238',\n",
       " 'v238',\n",
       " 'x239',\n",
       " 'y239',\n",
       " 'z239',\n",
       " 'v239',\n",
       " 'x240',\n",
       " 'y240',\n",
       " 'z240',\n",
       " 'v240',\n",
       " 'x241',\n",
       " 'y241',\n",
       " 'z241',\n",
       " 'v241',\n",
       " 'x242',\n",
       " 'y242',\n",
       " 'z242',\n",
       " 'v242',\n",
       " 'x243',\n",
       " 'y243',\n",
       " 'z243',\n",
       " 'v243',\n",
       " 'x244',\n",
       " 'y244',\n",
       " 'z244',\n",
       " 'v244',\n",
       " 'x245',\n",
       " 'y245',\n",
       " 'z245',\n",
       " 'v245',\n",
       " 'x246',\n",
       " 'y246',\n",
       " 'z246',\n",
       " 'v246',\n",
       " 'x247',\n",
       " 'y247',\n",
       " 'z247',\n",
       " 'v247',\n",
       " 'x248',\n",
       " 'y248',\n",
       " 'z248',\n",
       " 'v248',\n",
       " 'x249',\n",
       " 'y249',\n",
       " 'z249',\n",
       " 'v249',\n",
       " 'x250',\n",
       " 'y250',\n",
       " 'z250',\n",
       " ...]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coords.csv', mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"sad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks (use FACEMESH_TESSELATION)\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp.solutions.holistic.FACEMESH_TESSELATION, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            # Concatenate rows\n",
    "            row = pose_row + face_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            \n",
    "            # Export to CSV\n",
    "            with open('coords.csv', mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row) \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Custom Model Using Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read in Collected Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>v2</th>\n",
       "      <th>x3</th>\n",
       "      <th>...</th>\n",
       "      <th>z499</th>\n",
       "      <th>v499</th>\n",
       "      <th>x500</th>\n",
       "      <th>y500</th>\n",
       "      <th>z500</th>\n",
       "      <th>v500</th>\n",
       "      <th>x501</th>\n",
       "      <th>y501</th>\n",
       "      <th>z501</th>\n",
       "      <th>v501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bored</td>\n",
       "      <td>0.629020</td>\n",
       "      <td>0.568639</td>\n",
       "      <td>-1.385486</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.658297</td>\n",
       "      <td>0.489352</td>\n",
       "      <td>-1.307458</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>0.676192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713972</td>\n",
       "      <td>0.468259</td>\n",
       "      <td>0.015721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.720104</td>\n",
       "      <td>0.459737</td>\n",
       "      <td>0.016592</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bored</td>\n",
       "      <td>0.628555</td>\n",
       "      <td>0.568766</td>\n",
       "      <td>-1.359292</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.658376</td>\n",
       "      <td>0.489075</td>\n",
       "      <td>-1.280529</td>\n",
       "      <td>0.999895</td>\n",
       "      <td>0.676367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717927</td>\n",
       "      <td>0.469455</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.724083</td>\n",
       "      <td>0.461155</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bored</td>\n",
       "      <td>0.628529</td>\n",
       "      <td>0.568950</td>\n",
       "      <td>-1.306886</td>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.658665</td>\n",
       "      <td>0.489102</td>\n",
       "      <td>-1.224599</td>\n",
       "      <td>0.999901</td>\n",
       "      <td>0.676657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.718667</td>\n",
       "      <td>0.468024</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.724801</td>\n",
       "      <td>0.460089</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bored</td>\n",
       "      <td>0.628614</td>\n",
       "      <td>0.569653</td>\n",
       "      <td>-1.302280</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>0.659160</td>\n",
       "      <td>0.489213</td>\n",
       "      <td>-1.221179</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.677106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.718511</td>\n",
       "      <td>0.468370</td>\n",
       "      <td>0.022802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.724580</td>\n",
       "      <td>0.460204</td>\n",
       "      <td>0.024055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bored</td>\n",
       "      <td>0.628584</td>\n",
       "      <td>0.570388</td>\n",
       "      <td>-1.246793</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.659290</td>\n",
       "      <td>0.489301</td>\n",
       "      <td>-1.169416</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.677280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.719794</td>\n",
       "      <td>0.464793</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.725901</td>\n",
       "      <td>0.456046</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class        x1        y1        z1        v1        x2        y2  \\\n",
       "0  bored  0.629020  0.568639 -1.385486  0.999910  0.658297  0.489352   \n",
       "1  bored  0.628555  0.568766 -1.359292  0.999915  0.658376  0.489075   \n",
       "2  bored  0.628529  0.568950 -1.306886  0.999920  0.658665  0.489102   \n",
       "3  bored  0.628614  0.569653 -1.302280  0.999924  0.659160  0.489213   \n",
       "4  bored  0.628584  0.570388 -1.246793  0.999929  0.659290  0.489301   \n",
       "\n",
       "         z2        v2        x3  ...      z499  v499      x500      y500  \\\n",
       "0 -1.307458  0.999890  0.676192  ... -0.016321   0.0  0.713972  0.468259   \n",
       "1 -1.280529  0.999895  0.676367  ... -0.015541   0.0  0.717927  0.469455   \n",
       "2 -1.224599  0.999901  0.676657  ... -0.015379   0.0  0.718667  0.468024   \n",
       "3 -1.221179  0.999906  0.677106  ... -0.012318   0.0  0.718511  0.468370   \n",
       "4 -1.169416  0.999911  0.677280  ... -0.013870   0.0  0.719794  0.464793   \n",
       "\n",
       "       z500  v500      x501      y501      z501  v501  \n",
       "0  0.015721   0.0  0.720104  0.459737  0.016592   0.0  \n",
       "1  0.017143   0.0  0.724083  0.461155  0.018116   0.0  \n",
       "2  0.017193   0.0  0.724801  0.460089  0.018118   0.0  \n",
       "3  0.022802   0.0  0.724580  0.460204  0.024055   0.0  \n",
       "4  0.020200   0.0  0.725901  0.456046  0.021383   0.0  \n",
       "\n",
       "[5 rows x 2005 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>v2</th>\n",
       "      <th>x3</th>\n",
       "      <th>...</th>\n",
       "      <th>z499</th>\n",
       "      <th>v499</th>\n",
       "      <th>x500</th>\n",
       "      <th>y500</th>\n",
       "      <th>z500</th>\n",
       "      <th>v500</th>\n",
       "      <th>x501</th>\n",
       "      <th>y501</th>\n",
       "      <th>z501</th>\n",
       "      <th>v501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.547834</td>\n",
       "      <td>0.554001</td>\n",
       "      <td>-1.346296</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.577068</td>\n",
       "      <td>0.491668</td>\n",
       "      <td>-1.270525</td>\n",
       "      <td>0.998378</td>\n",
       "      <td>0.593076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620441</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626484</td>\n",
       "      <td>0.484749</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.547324</td>\n",
       "      <td>0.555652</td>\n",
       "      <td>-1.306115</td>\n",
       "      <td>0.999271</td>\n",
       "      <td>0.577012</td>\n",
       "      <td>0.492059</td>\n",
       "      <td>-1.237636</td>\n",
       "      <td>0.998449</td>\n",
       "      <td>0.593033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617775</td>\n",
       "      <td>0.487227</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623953</td>\n",
       "      <td>0.480202</td>\n",
       "      <td>0.017936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.546696</td>\n",
       "      <td>0.557833</td>\n",
       "      <td>-1.306317</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.576753</td>\n",
       "      <td>0.492594</td>\n",
       "      <td>-1.237203</td>\n",
       "      <td>0.998476</td>\n",
       "      <td>0.592998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617015</td>\n",
       "      <td>0.486855</td>\n",
       "      <td>0.014468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623224</td>\n",
       "      <td>0.479893</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.545958</td>\n",
       "      <td>0.558138</td>\n",
       "      <td>-1.305457</td>\n",
       "      <td>0.999269</td>\n",
       "      <td>0.576405</td>\n",
       "      <td>0.492615</td>\n",
       "      <td>-1.236949</td>\n",
       "      <td>0.998491</td>\n",
       "      <td>0.592972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619035</td>\n",
       "      <td>0.487489</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625227</td>\n",
       "      <td>0.480185</td>\n",
       "      <td>0.014181</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.545848</td>\n",
       "      <td>0.558433</td>\n",
       "      <td>-1.313607</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.576298</td>\n",
       "      <td>0.492713</td>\n",
       "      <td>-1.243711</td>\n",
       "      <td>0.998481</td>\n",
       "      <td>0.593066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619150</td>\n",
       "      <td>0.487931</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625306</td>\n",
       "      <td>0.480583</td>\n",
       "      <td>0.015057</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class        x1        y1        z1        v1        x2        y2  \\\n",
       "297   sad  0.547834  0.554001 -1.346296  0.999237  0.577068  0.491668   \n",
       "298   sad  0.547324  0.555652 -1.306115  0.999271  0.577012  0.492059   \n",
       "299   sad  0.546696  0.557833 -1.306317  0.999268  0.576753  0.492594   \n",
       "300   sad  0.545958  0.558138 -1.305457  0.999269  0.576405  0.492615   \n",
       "301   sad  0.545848  0.558433 -1.313607  0.999252  0.576298  0.492713   \n",
       "\n",
       "           z2        v2        x3  ...      z499  v499      x500      y500  \\\n",
       "297 -1.270525  0.998378  0.593076  ... -0.001200   0.0  0.620441  0.491370   \n",
       "298 -1.237636  0.998449  0.593033  ... -0.004037   0.0  0.617775  0.487227   \n",
       "299 -1.237203  0.998476  0.592998  ... -0.005649   0.0  0.617015  0.486855   \n",
       "300 -1.236949  0.998491  0.592972  ... -0.006536   0.0  0.619035  0.487489   \n",
       "301 -1.243711  0.998481  0.593066  ... -0.006081   0.0  0.619150  0.487931   \n",
       "\n",
       "         z500  v500      x501      y501      z501  v501  \n",
       "297  0.023442   0.0  0.626484  0.484749  0.024353   0.0  \n",
       "298  0.017349   0.0  0.623953  0.480202  0.017936   0.0  \n",
       "299  0.014468   0.0  0.623224  0.479893  0.014918   0.0  \n",
       "300  0.013710   0.0  0.625227  0.480185  0.014181   0.0  \n",
       "301  0.014544   0.0  0.625306  0.480583  0.015057   0.0  \n",
       "\n",
       "[5 rows x 2005 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>v2</th>\n",
       "      <th>x3</th>\n",
       "      <th>...</th>\n",
       "      <th>z499</th>\n",
       "      <th>v499</th>\n",
       "      <th>x500</th>\n",
       "      <th>y500</th>\n",
       "      <th>z500</th>\n",
       "      <th>v500</th>\n",
       "      <th>x501</th>\n",
       "      <th>y501</th>\n",
       "      <th>z501</th>\n",
       "      <th>v501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 2005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [class, x1, y1, z1, v1, x2, y2, z2, v2, x3, y3, z3, v3, x4, y4, z4, v4, x5, y5, z5, v5, x6, y6, z6, v6, x7, y7, z7, v7, x8, y8, z8, v8, x9, y9, z9, v9, x10, y10, z10, v10, x11, y11, z11, v11, x12, y12, z12, v12, x13, y13, z13, v13, x14, y14, z14, v14, x15, y15, z15, v15, x16, y16, z16, v16, x17, y17, z17, v17, x18, y18, z18, v18, x19, y19, z19, v19, x20, y20, z20, v20, x21, y21, z21, v21, x22, y22, z22, v22, x23, y23, z23, v23, x24, y24, z24, v24, x25, y25, z25, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 2005 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['class']=='wave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1) # features\n",
    "y = df['class'] # target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>v2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>...</th>\n",
       "      <th>z499</th>\n",
       "      <th>v499</th>\n",
       "      <th>x500</th>\n",
       "      <th>y500</th>\n",
       "      <th>z500</th>\n",
       "      <th>v500</th>\n",
       "      <th>x501</th>\n",
       "      <th>y501</th>\n",
       "      <th>z501</th>\n",
       "      <th>v501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.529031</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>-1.208879</td>\n",
       "      <td>0.999475</td>\n",
       "      <td>0.559137</td>\n",
       "      <td>0.480468</td>\n",
       "      <td>-1.138925</td>\n",
       "      <td>0.998923</td>\n",
       "      <td>0.582513</td>\n",
       "      <td>0.481862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590233</td>\n",
       "      <td>0.468628</td>\n",
       "      <td>0.020492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.595983</td>\n",
       "      <td>0.461490</td>\n",
       "      <td>0.021075</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.392653</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>-1.430887</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.422583</td>\n",
       "      <td>0.490989</td>\n",
       "      <td>-1.331480</td>\n",
       "      <td>0.999476</td>\n",
       "      <td>0.440784</td>\n",
       "      <td>0.495545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474671</td>\n",
       "      <td>0.503379</td>\n",
       "      <td>0.003644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481827</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.559856</td>\n",
       "      <td>0.604692</td>\n",
       "      <td>-1.537517</td>\n",
       "      <td>0.998916</td>\n",
       "      <td>0.585246</td>\n",
       "      <td>0.524771</td>\n",
       "      <td>-1.457976</td>\n",
       "      <td>0.997477</td>\n",
       "      <td>0.600971</td>\n",
       "      <td>0.523674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635077</td>\n",
       "      <td>0.517787</td>\n",
       "      <td>0.017497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.639931</td>\n",
       "      <td>0.511390</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.520391</td>\n",
       "      <td>0.588952</td>\n",
       "      <td>-1.338330</td>\n",
       "      <td>0.999192</td>\n",
       "      <td>0.553459</td>\n",
       "      <td>0.508157</td>\n",
       "      <td>-1.278191</td>\n",
       "      <td>0.998464</td>\n",
       "      <td>0.576750</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.594599</td>\n",
       "      <td>0.517553</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600545</td>\n",
       "      <td>0.508007</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.561019</td>\n",
       "      <td>0.605435</td>\n",
       "      <td>-1.474053</td>\n",
       "      <td>0.998663</td>\n",
       "      <td>0.585978</td>\n",
       "      <td>0.525987</td>\n",
       "      <td>-1.401680</td>\n",
       "      <td>0.996999</td>\n",
       "      <td>0.601071</td>\n",
       "      <td>0.525276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635474</td>\n",
       "      <td>0.519326</td>\n",
       "      <td>0.012322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.640690</td>\n",
       "      <td>0.512325</td>\n",
       "      <td>0.012918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.371020</td>\n",
       "      <td>0.545866</td>\n",
       "      <td>-1.415979</td>\n",
       "      <td>0.999313</td>\n",
       "      <td>0.410275</td>\n",
       "      <td>0.486595</td>\n",
       "      <td>-1.311429</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.430361</td>\n",
       "      <td>0.492007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.457727</td>\n",
       "      <td>0.509061</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464680</td>\n",
       "      <td>0.501833</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.518099</td>\n",
       "      <td>0.543335</td>\n",
       "      <td>-1.290829</td>\n",
       "      <td>0.999371</td>\n",
       "      <td>0.547564</td>\n",
       "      <td>0.482061</td>\n",
       "      <td>-1.218027</td>\n",
       "      <td>0.998729</td>\n",
       "      <td>0.570402</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604905</td>\n",
       "      <td>0.480979</td>\n",
       "      <td>0.030421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.610917</td>\n",
       "      <td>0.475190</td>\n",
       "      <td>0.031543</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.522656</td>\n",
       "      <td>0.580285</td>\n",
       "      <td>-1.336037</td>\n",
       "      <td>0.999226</td>\n",
       "      <td>0.555130</td>\n",
       "      <td>0.502012</td>\n",
       "      <td>-1.273325</td>\n",
       "      <td>0.998561</td>\n",
       "      <td>0.578338</td>\n",
       "      <td>0.499466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.594865</td>\n",
       "      <td>0.516136</td>\n",
       "      <td>-0.000913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600881</td>\n",
       "      <td>0.505847</td>\n",
       "      <td>-0.000949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.628595</td>\n",
       "      <td>0.573134</td>\n",
       "      <td>-1.218545</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.660316</td>\n",
       "      <td>0.490308</td>\n",
       "      <td>-1.145461</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.678244</td>\n",
       "      <td>0.490302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.720123</td>\n",
       "      <td>0.471514</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.726254</td>\n",
       "      <td>0.463033</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.394310</td>\n",
       "      <td>0.558482</td>\n",
       "      <td>-1.419492</td>\n",
       "      <td>0.999599</td>\n",
       "      <td>0.423687</td>\n",
       "      <td>0.488945</td>\n",
       "      <td>-1.329708</td>\n",
       "      <td>0.999516</td>\n",
       "      <td>0.441654</td>\n",
       "      <td>0.493293</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.501491</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485358</td>\n",
       "      <td>0.493776</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        y1        z1        v1        x2        y2        z2  \\\n",
       "263  0.529031  0.539838 -1.208879  0.999475  0.559137  0.480468 -1.138925   \n",
       "77   0.392653  0.559100 -1.430887  0.999548  0.422583  0.490989 -1.331480   \n",
       "163  0.559856  0.604692 -1.537517  0.998916  0.585246  0.524771 -1.457976   \n",
       "206  0.520391  0.588952 -1.338330  0.999192  0.553459  0.508157 -1.278191   \n",
       "167  0.561019  0.605435 -1.474053  0.998663  0.585978  0.525987 -1.401680   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "51   0.371020  0.545866 -1.415979  0.999313  0.410275  0.486595 -1.311429   \n",
       "284  0.518099  0.543335 -1.290829  0.999371  0.547564  0.482061 -1.218027   \n",
       "205  0.522656  0.580285 -1.336037  0.999226  0.555130  0.502012 -1.273325   \n",
       "9    0.628595  0.573134 -1.218545  0.999945  0.660316  0.490308 -1.145461   \n",
       "83   0.394310  0.558482 -1.419492  0.999599  0.423687  0.488945 -1.329708   \n",
       "\n",
       "           v2        x3        y3  ...      z499  v499      x500      y500  \\\n",
       "263  0.998923  0.582513  0.481862  ... -0.001608   0.0  0.590233  0.468628   \n",
       "77   0.999476  0.440784  0.495545  ... -0.016099   0.0  0.474671  0.503379   \n",
       "163  0.997477  0.600971  0.523674  ... -0.013425   0.0  0.635077  0.517787   \n",
       "206  0.998464  0.576750  0.505051  ... -0.014673   0.0  0.594599  0.517553   \n",
       "167  0.996999  0.601071  0.525276  ... -0.015614   0.0  0.635474  0.519326   \n",
       "..        ...       ...       ...  ...       ...   ...       ...       ...   \n",
       "51   0.999225  0.430361  0.492007  ... -0.014271   0.0  0.457727  0.509061   \n",
       "284  0.998729  0.570402  0.482558  ...  0.002969   0.0  0.604905  0.480979   \n",
       "205  0.998561  0.578338  0.499466  ... -0.015013   0.0  0.594865  0.516136   \n",
       "9    0.999928  0.678244  0.490302  ... -0.014698   0.0  0.720123  0.471514   \n",
       "83   0.999516  0.441654  0.493293  ... -0.015838   0.0  0.478600  0.501491   \n",
       "\n",
       "         z500  v500      x501      y501      z501  v501  \n",
       "263  0.020492   0.0  0.595983  0.461490  0.021075   0.0  \n",
       "77   0.003644   0.0  0.481827  0.494505  0.003946   0.0  \n",
       "163  0.017497   0.0  0.639931  0.511390  0.018394   0.0  \n",
       "206  0.000154   0.0  0.600545  0.508007  0.000044   0.0  \n",
       "167  0.012322   0.0  0.640690  0.512325  0.012918   0.0  \n",
       "..        ...   ...       ...       ...       ...   ...  \n",
       "51   0.008053   0.0  0.464680  0.501833  0.008377   0.0  \n",
       "284  0.030421   0.0  0.610917  0.475190  0.031543   0.0  \n",
       "205 -0.000913   0.0  0.600881  0.505847 -0.000949   0.0  \n",
       "9    0.018277   0.0  0.726254  0.463033  0.019299   0.0  \n",
       "83   0.004626   0.0  0.485358  0.493776  0.004819   0.0  \n",
       "\n",
       "[91 rows x 2004 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train Machine Learning Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression())]),\n",
       " 'rc': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('ridgeclassifier', RidgeClassifier())]),\n",
       " 'rf': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('randomforestclassifier', RandomForestClassifier())]),\n",
       " 'gb': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('gradientboostingclassifier', GradientBoostingClassifier())])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sad', 'bored', 'sad', 'sad', 'sad', 'bored', 'sad', 'sad', 'sad',\n",
       "       'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad',\n",
       "       'bored', 'bored', 'bored', 'bored', 'bored', 'sad', 'bored',\n",
       "       'bored', 'sad', 'sad', 'sad', 'bored', 'bored', 'bored', 'bored',\n",
       "       'sad', 'bored', 'sad', 'sad', 'sad', 'sad', 'bored', 'sad', 'sad',\n",
       "       'bored', 'sad', 'sad', 'bored', 'bored', 'sad', 'sad', 'sad',\n",
       "       'bored', 'sad', 'sad', 'sad', 'sad', 'bored', 'sad', 'bored',\n",
       "       'sad', 'bored', 'bored', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad',\n",
       "       'bored', 'bored', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad',\n",
       "       'sad', 'bored', 'bored', 'bored', 'sad', 'sad', 'sad', 'bored',\n",
       "       'bored', 'sad', 'bored', 'sad', 'sad', 'bored', 'bored'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rc'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate and Serialize Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0\n",
      "rc 1.0\n",
      "rf 1.0\n",
      "gb 1.0\n"
     ]
    }
   ],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sad', 'bored', 'sad', 'sad', 'sad', 'bored', 'sad', 'sad', 'sad',\n",
       "       'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad',\n",
       "       'bored', 'bored', 'bored', 'bored', 'bored', 'sad', 'bored',\n",
       "       'bored', 'sad', 'sad', 'sad', 'bored', 'bored', 'bored', 'bored',\n",
       "       'sad', 'bored', 'sad', 'sad', 'sad', 'sad', 'bored', 'sad', 'sad',\n",
       "       'bored', 'sad', 'sad', 'bored', 'bored', 'sad', 'sad', 'sad',\n",
       "       'bored', 'sad', 'sad', 'sad', 'sad', 'bored', 'sad', 'bored',\n",
       "       'sad', 'bored', 'bored', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad',\n",
       "       'bored', 'bored', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad',\n",
       "       'sad', 'bored', 'bored', 'bored', 'sad', 'sad', 'sad', 'bored',\n",
       "       'bored', 'sad', 'bored', 'sad', 'sad', 'bored', 'bored'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rc'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263      sad\n",
       "77     bored\n",
       "163      sad\n",
       "206      sad\n",
       "167      sad\n",
       "       ...  \n",
       "51     bored\n",
       "284      sad\n",
       "205      sad\n",
       "9      bored\n",
       "83     bored\n",
       "Name: class, Length: 91, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Make Detections with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks (use FACEMESH_TESSELATION)\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp.solutions.holistic.FACEMESH_TESSELATION, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            # Concatenate rows\n",
    "            row = pose_row + face_row\n",
    "            \n",
    "            # Make Detections\n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            print(body_language_class, body_language_prob)\n",
    "            \n",
    "            # Grab ear coords\n",
    "            coords = tuple(np.multiply(\n",
    "                            np.array(\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                        , [640,480]).astype(int))\n",
    "            \n",
    "            cv2.rectangle(image, \n",
    "                          (coords[0], coords[1]+5), \n",
    "                          (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                          (245, 117, 16), -1)\n",
    "            cv2.putText(image, body_language_class, coords, \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Get status box\n",
    "            cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            # Display Class\n",
    "            cv2.putText(image, 'CLASS', (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, body_language_class.split(' ')[0], (90,40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display Probability\n",
    "            cv2.putText(image, 'PROB', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2)), \n",
    "                        (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408, 132)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(np.multiply(np.array((results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y)), [640,480]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x352 2 persons, 221.4ms\n",
      "Speed: 15.9ms preprocess, 221.4ms inference, 15.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 189.7ms\n",
      "Speed: 4.9ms preprocess, 189.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 163.8ms\n",
      "Speed: 8.0ms preprocess, 163.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 149.4ms\n",
      "Speed: 0.0ms preprocess, 149.4ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 133.0ms\n",
      "Speed: 0.0ms preprocess, 133.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 121.2ms\n",
      "Speed: 5.9ms preprocess, 121.2ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 132.7ms\n",
      "Speed: 0.0ms preprocess, 132.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 145.3ms\n",
      "Speed: 4.1ms preprocess, 145.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 177.6ms\n",
      "Speed: 9.9ms preprocess, 177.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 207.4ms\n",
      "Speed: 8.7ms preprocess, 207.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 152.4ms\n",
      "Speed: 8.1ms preprocess, 152.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 167.0ms\n",
      "Speed: 6.9ms preprocess, 167.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 284.2ms\n",
      "Speed: 11.4ms preprocess, 284.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 156.6ms\n",
      "Speed: 0.0ms preprocess, 156.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 193.5ms\n",
      "Speed: 15.3ms preprocess, 193.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 176.0ms\n",
      "Speed: 4.7ms preprocess, 176.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 210.5ms\n",
      "Speed: 2.1ms preprocess, 210.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 216.0ms\n",
      "Speed: 0.0ms preprocess, 216.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 227.8ms\n",
      "Speed: 8.4ms preprocess, 227.8ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 247.5ms\n",
      "Speed: 7.4ms preprocess, 247.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 183.7ms\n",
      "Speed: 7.9ms preprocess, 183.7ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 146.8ms\n",
      "Speed: 2.8ms preprocess, 146.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 135.8ms\n",
      "Speed: 2.1ms preprocess, 135.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 131.3ms\n",
      "Speed: 3.4ms preprocess, 131.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 144.1ms\n",
      "Speed: 0.0ms preprocess, 144.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 1 person, 131.4ms\n",
      "Speed: 0.0ms preprocess, 131.4ms inference, 13.8ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 126.5ms\n",
      "Speed: 3.6ms preprocess, 126.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 1 person, 129.1ms\n",
      "Speed: 0.0ms preprocess, 129.1ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 145.7ms\n",
      "Speed: 0.0ms preprocess, 145.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 134.6ms\n",
      "Speed: 0.0ms preprocess, 134.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 161.6ms\n",
      "Speed: 3.7ms preprocess, 161.6ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 130.2ms\n",
      "Speed: 6.6ms preprocess, 130.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 131.1ms\n",
      "Speed: 8.2ms preprocess, 131.1ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 121.6ms\n",
      "Speed: 1.6ms preprocess, 121.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 137.0ms\n",
      "Speed: 0.0ms preprocess, 137.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 1 person, 133.1ms\n",
      "Speed: 0.0ms preprocess, 133.1ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 118.0ms\n",
      "Speed: 4.0ms preprocess, 118.0ms inference, 12.4ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 141.6ms\n",
      "Speed: 5.5ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 144.3ms\n",
      "Speed: 2.7ms preprocess, 144.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 131.8ms\n",
      "Speed: 3.9ms preprocess, 131.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 158.7ms\n",
      "Speed: 14.0ms preprocess, 158.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 132.6ms\n",
      "Speed: 0.0ms preprocess, 132.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 124.7ms\n",
      "Speed: 0.0ms preprocess, 124.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 118.7ms\n",
      "Speed: 5.3ms preprocess, 118.7ms inference, 11.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 130.8ms\n",
      "Speed: 3.4ms preprocess, 130.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 141.1ms\n",
      "Speed: 4.5ms preprocess, 141.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 128.3ms\n",
      "Speed: 5.1ms preprocess, 128.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 127.6ms\n",
      "Speed: 0.0ms preprocess, 127.6ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 116.2ms\n",
      "Speed: 2.9ms preprocess, 116.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 3 persons, 128.5ms\n",
      "Speed: 0.0ms preprocess, 128.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 122.3ms\n",
      "Speed: 3.3ms preprocess, 122.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 151.4ms\n",
      "Speed: 4.8ms preprocess, 151.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 148.6ms\n",
      "Speed: 3.0ms preprocess, 148.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 174.6ms\n",
      "Speed: 7.0ms preprocess, 174.6ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 158.6ms\n",
      "Speed: 4.5ms preprocess, 158.6ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x352 2 persons, 150.8ms\n",
      "Speed: 0.0ms preprocess, 150.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize Mediapipe pose model and drawing utilities\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load YOLOv8 model for person detection\n",
    "yolo_model = YOLO('yolov8n.pt')  # Use 'yolov8n.pt' for lightweight model\n",
    "\n",
    "# Path to input video\n",
    "video_path = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\output_videos\\Vid2.mp4'\n",
    "\n",
    "# Specify output folder and file\n",
    "output_folder = 'output_videos'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_video_path = os.path.join(output_folder, r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\output_videos\\Vnnbnhg.mp4')\n",
    "\n",
    "# Start capturing video from the file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get frame width, height, and FPS for saving the output video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initiate pose model\n",
    "with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=1) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Use YOLO to detect people in the frame\n",
    "        results = yolo_model(frame)\n",
    "        detections = results[0]  # Get detections from YOLO\n",
    "        \n",
    "        # Process each person detected\n",
    "        for det in detections.boxes:\n",
    "            if det.cls == 0:  # Class 0 is for 'person' in YOLO\n",
    "                # Get bounding box coordinates for each person\n",
    "                x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())\n",
    "\n",
    "                # Extract the person from the frame using the bounding box\n",
    "                person = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Convert to RGB for Mediapipe processing\n",
    "                person_rgb = cv2.cvtColor(person, cv2.COLOR_BGR2RGB)\n",
    "                person_rgb.flags.writeable = False\n",
    "\n",
    "                # Apply Mediapipe Pose estimation\n",
    "                pose_results = pose.process(person_rgb)\n",
    "                \n",
    "                # Recolor the image back to BGR for display\n",
    "                person_rgb.flags.writeable = True\n",
    "                person_bgr = cv2.cvtColor(person_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Draw Pose landmarks on the person\n",
    "                if pose_results.pose_landmarks:\n",
    "                    mp_drawing.draw_landmarks(person_bgr, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "\n",
    "                # Replace the processed person back into the original frame\n",
    "                frame[y1:y2, x1:x2] = person_bgr\n",
    "        \n",
    "        # Write the processed frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the processed frame (optional)\n",
    "        cv2.imshow('Processed Video Feed', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and writer objects, and close windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Initialize mediapipe holistic model and drawing utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path to input video\n",
    "video_path = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_120.jpg'\n",
    "\n",
    "# Specify output folder and file\n",
    "output_folder = 'output_videos'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_video_path = os.path.join(output_folder, r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\Vid3Processed.mp4')\n",
    "\n",
    "# Start capturing video from the file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get frame width, height, and FPS for saving the output video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed to RGB for Mediapipe processing\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, \n",
    "                                      mp.solutions.holistic.FACEMESH_TESSELATION, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # Write the processed frame to the output video\n",
    "        out.write(image)\n",
    "        \n",
    "        # Show the processed image (optional)\n",
    "        cv2.imshow('Processed Video Feed', image)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and writer objects, and close windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "landmark {\n",
       "  x: 0.07610340416431427\n",
       "  y: -0.5598576068878174\n",
       "  z: -0.23969310522079468\n",
       "  visibility: 0.9999986886978149\n",
       "}\n",
       "landmark {\n",
       "  x: 0.08138136565685272\n",
       "  y: -0.6002929210662842\n",
       "  z: -0.22783206403255463\n",
       "  visibility: 0.9999979734420776\n",
       "}\n",
       "landmark {\n",
       "  x: 0.08076263964176178\n",
       "  y: -0.6028578877449036\n",
       "  z: -0.22877252101898193\n",
       "  visibility: 0.999998927116394\n",
       "}\n",
       "landmark {\n",
       "  x: 0.08007578551769257\n",
       "  y: -0.603441596031189\n",
       "  z: -0.22994594275951385\n",
       "  visibility: 0.9999978542327881\n",
       "}\n",
       "landmark {\n",
       "  x: 0.044503096491098404\n",
       "  y: -0.6005644798278809\n",
       "  z: -0.229008749127388\n",
       "  visibility: 0.9999973773956299\n",
       "}\n",
       "landmark {\n",
       "  x: 0.045958511531353\n",
       "  y: -0.5998915433883667\n",
       "  z: -0.2325519174337387\n",
       "  visibility: 0.9999980926513672\n",
       "}\n",
       "landmark {\n",
       "  x: 0.045850515365600586\n",
       "  y: -0.601276695728302\n",
       "  z: -0.23145847022533417\n",
       "  visibility: 0.999996542930603\n",
       "}\n",
       "landmark {\n",
       "  x: 0.13288669288158417\n",
       "  y: -0.6102220416069031\n",
       "  z: -0.12780320644378662\n",
       "  visibility: 0.9999990463256836\n",
       "}\n",
       "landmark {\n",
       "  x: -0.014122351072728634\n",
       "  y: -0.6016106605529785\n",
       "  z: -0.11905825138092041\n",
       "  visibility: 0.9999990463256836\n",
       "}\n",
       "landmark {\n",
       "  x: 0.10221484303474426\n",
       "  y: -0.5498636364936829\n",
       "  z: -0.2021077573299408\n",
       "  visibility: 0.9999988079071045\n",
       "}\n",
       "landmark {\n",
       "  x: 0.05737069621682167\n",
       "  y: -0.5460495352745056\n",
       "  z: -0.19474172592163086\n",
       "  visibility: 0.9999984502792358\n",
       "}\n",
       "landmark {\n",
       "  x: 0.2387656569480896\n",
       "  y: -0.43978142738342285\n",
       "  z: 0.0032836729660630226\n",
       "  visibility: 0.999992847442627\n",
       "}\n",
       "landmark {\n",
       "  x: -0.1007128357887268\n",
       "  y: -0.44342172145843506\n",
       "  z: -0.0064954799599945545\n",
       "  visibility: 0.999975323677063\n",
       "}\n",
       "landmark {\n",
       "  x: 0.2276318073272705\n",
       "  y: -0.1706807017326355\n",
       "  z: -0.07737477123737335\n",
       "  visibility: 0.9999313354492188\n",
       "}\n",
       "landmark {\n",
       "  x: -0.20581471920013428\n",
       "  y: -0.23830388486385345\n",
       "  z: -0.05715636536478996\n",
       "  visibility: 0.999823272228241\n",
       "}\n",
       "landmark {\n",
       "  x: 0.17416170239448547\n",
       "  y: -0.15757010877132416\n",
       "  z: -0.24349641799926758\n",
       "  visibility: 0.9999184608459473\n",
       "}\n",
       "landmark {\n",
       "  x: -0.0850880816578865\n",
       "  y: -0.12419513612985611\n",
       "  z: -0.25906211137771606\n",
       "  visibility: 0.9997099041938782\n",
       "}\n",
       "landmark {\n",
       "  x: 0.15433937311172485\n",
       "  y: -0.10955636203289032\n",
       "  z: -0.30825507640838623\n",
       "  visibility: 0.9997227787971497\n",
       "}\n",
       "landmark {\n",
       "  x: -0.03118288703262806\n",
       "  y: -0.03364238888025284\n",
       "  z: -0.31331002712249756\n",
       "  visibility: 0.998519241809845\n",
       "}\n",
       "landmark {\n",
       "  x: 0.13083451986312866\n",
       "  y: -0.14872772991657257\n",
       "  z: -0.33432915806770325\n",
       "  visibility: 0.9997370839118958\n",
       "}\n",
       "landmark {\n",
       "  x: 0.014506518840789795\n",
       "  y: -0.08669967204332352\n",
       "  z: -0.3333532214164734\n",
       "  visibility: 0.9987266659736633\n",
       "}\n",
       "landmark {\n",
       "  x: 0.15789110958576202\n",
       "  y: -0.16334134340286255\n",
       "  z: -0.24560725688934326\n",
       "  visibility: 0.9997510313987732\n",
       "}\n",
       "landmark {\n",
       "  x: -0.05017771199345589\n",
       "  y: -0.12171614915132523\n",
       "  z: -0.273192435503006\n",
       "  visibility: 0.999032735824585\n",
       "}\n",
       "landmark {\n",
       "  x: 0.11462539434432983\n",
       "  y: 0.005526603665202856\n",
       "  z: -0.004484702832996845\n",
       "  visibility: 0.9999802112579346\n",
       "}\n",
       "landmark {\n",
       "  x: -0.11541374027729034\n",
       "  y: -0.0038665421307086945\n",
       "  z: 0.0036750107537955046\n",
       "  visibility: 0.9999744892120361\n",
       "}\n",
       "landmark {\n",
       "  x: 0.0677148848772049\n",
       "  y: 0.24892129004001617\n",
       "  z: -0.0358133427798748\n",
       "  visibility: 0.7583569884300232\n",
       "}\n",
       "landmark {\n",
       "  x: -0.17489032447338104\n",
       "  y: 0.33171945810317993\n",
       "  z: -0.14021039009094238\n",
       "  visibility: 0.5227707028388977\n",
       "}\n",
       "landmark {\n",
       "  x: 0.015135593712329865\n",
       "  y: 0.4511164724826813\n",
       "  z: 0.11549529433250427\n",
       "  visibility: 0.00027400601538829505\n",
       "}\n",
       "landmark {\n",
       "  x: -0.13150489330291748\n",
       "  y: 0.46454891562461853\n",
       "  z: 0.0050273858942091465\n",
       "  visibility: 0.00012930059165228158\n",
       "}\n",
       "landmark {\n",
       "  x: 0.020210372284054756\n",
       "  y: 0.4710119962692261\n",
       "  z: 0.13208602368831635\n",
       "  visibility: 0.001392287202179432\n",
       "}\n",
       "landmark {\n",
       "  x: -0.13711240887641907\n",
       "  y: 0.4765034019947052\n",
       "  z: 0.029736313968896866\n",
       "  visibility: 0.00041079800575971603\n",
       "}\n",
       "landmark {\n",
       "  x: -0.06296268850564957\n",
       "  y: 0.5927918553352356\n",
       "  z: 0.17633774876594543\n",
       "  visibility: 0.00017480766109656543\n",
       "}\n",
       "landmark {\n",
       "  x: -0.04426223039627075\n",
       "  y: 0.487650066614151\n",
       "  z: 0.051442719995975494\n",
       "  visibility: 0.00018053922394756228\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_world_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "landmark {\n",
       "  x: 0.5237988233566284\n",
       "  y: 0.3085106909275055\n",
       "  z: -0.009163235314190388\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5225407481193542\n",
       "  y: 0.2887094020843506\n",
       "  z: -0.016820957884192467\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5218188762664795\n",
       "  y: 0.29591765999794006\n",
       "  z: -0.00927744060754776\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5148445963859558\n",
       "  y: 0.2752120792865753\n",
       "  z: -0.01321321353316307\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5214449167251587\n",
       "  y: 0.2831488847732544\n",
       "  z: -0.01779015362262726\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5192997455596924\n",
       "  y: 0.276962548494339\n",
       "  z: -0.016528351232409477\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5133754014968872\n",
       "  y: 0.26295819878578186\n",
       "  z: -0.008182582445442677\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4890238642692566\n",
       "  y: 0.2814463973045349\n",
       "  z: -0.0008227388025261462\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5092921257019043\n",
       "  y: 0.24980776011943817\n",
       "  z: -0.005937241017818451\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5074960589408875\n",
       "  y: 0.24228627979755402\n",
       "  z: -0.00631817989051342\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5002334117889404\n",
       "  y: 0.21654316782951355\n",
       "  z: -0.0030911562498658895\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5241823792457581\n",
       "  y: 0.3112431466579437\n",
       "  z: -0.008787297643721104\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5242797136306763\n",
       "  y: 0.31350934505462646\n",
       "  z: -0.007876592688262463\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5240827202796936\n",
       "  y: 0.314586877822876\n",
       "  z: -0.006599151995033026\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5262625217437744\n",
       "  y: 0.32601818442344666\n",
       "  z: -0.003929086495190859\n",
       "}\n",
       "landmark {\n",
       "  x: 0.526969313621521\n",
       "  y: 0.3284526467323303\n",
       "  z: -0.00440073199570179\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5276543498039246\n",
       "  y: 0.3314332664012909\n",
       "  z: -0.004865629132837057\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5282183289527893\n",
       "  y: 0.33462297916412354\n",
       "  z: -0.004092948045581579\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5286329984664917\n",
       "  y: 0.33864983916282654\n",
       "  z: -0.0013084599049761891\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5226826071739197\n",
       "  y: 0.29206207394599915\n",
       "  z: -0.015325041487812996\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5190808176994324\n",
       "  y: 0.29488661885261536\n",
       "  z: -0.011709154583513737\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4685908555984497\n",
       "  y: 0.2700773775577545\n",
       "  z: 0.011279051192104816\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49925747513771057\n",
       "  y: 0.2787066400051117\n",
       "  z: -0.0024343132972717285\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4963814318180084\n",
       "  y: 0.28150197863578796\n",
       "  z: -0.0026962794363498688\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4932209849357605\n",
       "  y: 0.2834744453430176\n",
       "  z: -0.002357876393944025\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48772722482681274\n",
       "  y: 0.2840307354927063\n",
       "  z: -0.0006217102636583149\n",
       "}\n",
       "landmark {\n",
       "  x: 0.501346230506897\n",
       "  y: 0.27534016966819763\n",
       "  z: -0.0014208313077688217\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4916815161705017\n",
       "  y: 0.26647788286209106\n",
       "  z: -0.0041344258934259415\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4950999915599823\n",
       "  y: 0.2645516097545624\n",
       "  z: -0.003280215198174119\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48857203125953674\n",
       "  y: 0.2697969675064087\n",
       "  z: -0.0035968467127531767\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48672860860824585\n",
       "  y: 0.2737415134906769\n",
       "  z: -0.002648805733770132\n",
       "}\n",
       "landmark {\n",
       "  x: 0.485951691865921\n",
       "  y: 0.2889847159385681\n",
       "  z: 0.00047036621253937483\n",
       "}\n",
       "landmark {\n",
       "  x: 0.517964780330658\n",
       "  y: 0.35223662853240967\n",
       "  z: 5.4944252042332664e-05\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48729175329208374\n",
       "  y: 0.281220942735672\n",
       "  z: -7.206279860838549e-06\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4733608067035675\n",
       "  y: 0.2950233221054077\n",
       "  z: 0.012160653248429298\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4810943007469177\n",
       "  y: 0.2881390452384949\n",
       "  z: 0.003001013770699501\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5022587180137634\n",
       "  y: 0.30041345953941345\n",
       "  z: -0.0053631518967449665\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5192153453826904\n",
       "  y: 0.3110664486885071\n",
       "  z: -0.009506670758128166\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5202106833457947\n",
       "  y: 0.3165060877799988\n",
       "  z: -0.008154511451721191\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5145209431648254\n",
       "  y: 0.31665346026420593\n",
       "  z: -0.00834085512906313\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5113347172737122\n",
       "  y: 0.32129162549972534\n",
       "  z: -0.006063070148229599\n",
       "}\n",
       "landmark {\n",
       "  x: 0.516635537147522\n",
       "  y: 0.31971877813339233\n",
       "  z: -0.007343752775341272\n",
       "}\n",
       "landmark {\n",
       "  x: 0.513694167137146\n",
       "  y: 0.3225933611392975\n",
       "  z: -0.005234762094914913\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5089763402938843\n",
       "  y: 0.3358721137046814\n",
       "  z: -0.0007025865488685668\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5200223326683044\n",
       "  y: 0.29052287340164185\n",
       "  z: -0.01692938432097435\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5185235142707825\n",
       "  y: 0.28561562299728394\n",
       "  z: -0.017872164025902748\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4807564914226532\n",
       "  y: 0.2713320553302765\n",
       "  z: -0.0026192290242761374\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5057626366615295\n",
       "  y: 0.28281426429748535\n",
       "  z: -0.004594066645950079\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5107817053794861\n",
       "  y: 0.2964334785938263\n",
       "  z: -0.01093714963644743\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5095004439353943\n",
       "  y: 0.2943066358566284\n",
       "  z: -0.010049927048385143\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49199146032333374\n",
       "  y: 0.3076782822608948\n",
       "  z: -0.0034845999907702208\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5166597366333008\n",
       "  y: 0.2797382175922394\n",
       "  z: -0.016198202967643738\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48778122663497925\n",
       "  y: 0.2588452696800232\n",
       "  z: -0.0060369218699634075\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48350006341934204\n",
       "  y: 0.2644580006599426\n",
       "  z: -0.004578570835292339\n",
       "}\n",
       "landmark {\n",
       "  x: 0.47075340151786804\n",
       "  y: 0.2568570077419281\n",
       "  z: 0.0061896550469100475\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5024287700653076\n",
       "  y: 0.2536013722419739\n",
       "  z: -0.006374255288392305\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4982965290546417\n",
       "  y: 0.26460450887680054\n",
       "  z: -0.0019141429802402854\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5051318407058716\n",
       "  y: 0.3308428227901459\n",
       "  z: -0.0010636993683874607\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48769527673721313\n",
       "  y: 0.3456035256385803\n",
       "  z: 0.020424729213118553\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5136082172393799\n",
       "  y: 0.2978180944919586\n",
       "  z: -0.009380266070365906\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5164645314216614\n",
       "  y: 0.29770490527153015\n",
       "  z: -0.008965778164565563\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5076302886009216\n",
       "  y: 0.3290502727031708\n",
       "  z: -0.0008651887183077633\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5091878175735474\n",
       "  y: 0.32796546816825867\n",
       "  z: -0.0013891594717279077\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48060527443885803\n",
       "  y: 0.2622258961200714\n",
       "  z: -0.0034926238004118204\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5106680989265442\n",
       "  y: 0.2991832196712494\n",
       "  z: -0.009252738207578659\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4938525855541229\n",
       "  y: 0.25484582781791687\n",
       "  z: -0.006656222976744175\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4923122227191925\n",
       "  y: 0.25073525309562683\n",
       "  z: -0.007282706443220377\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4825803339481354\n",
       "  y: 0.2325461208820343\n",
       "  z: -0.0027517667040228844\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4758051037788391\n",
       "  y: 0.25936248898506165\n",
       "  z: 0.0007190379546955228\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48787108063697815\n",
       "  y: 0.24088560044765472\n",
       "  z: -0.004973193164914846\n",
       "}\n",
       "landmark {\n",
       "  x: 0.47766247391700745\n",
       "  y: 0.27055680751800537\n",
       "  z: -0.0004496009787544608\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4732467532157898\n",
       "  y: 0.270603209733963\n",
       "  z: 0.005166527349501848\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5197775959968567\n",
       "  y: 0.31421029567718506\n",
       "  z: -0.009159042499959469\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5157425999641418\n",
       "  y: 0.3185044527053833\n",
       "  z: -0.00794128980487585\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5125381350517273\n",
       "  y: 0.3219432532787323\n",
       "  z: -0.005905406083911657\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5144137144088745\n",
       "  y: 0.2984031140804291\n",
       "  z: -0.008428390137851238\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5084125399589539\n",
       "  y: 0.32852938771247864\n",
       "  z: -0.0011284861247986555\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5110374093055725\n",
       "  y: 0.33112284541130066\n",
       "  z: -0.0020369919948279858\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5096312165260315\n",
       "  y: 0.32749488949775696\n",
       "  z: -0.0013241785345599055\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5156651139259338\n",
       "  y: 0.2941856384277344\n",
       "  z: -0.013381551951169968\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5139865279197693\n",
       "  y: 0.32192733883857727\n",
       "  z: -0.004745609126985073\n",
       "}\n",
       "landmark {\n",
       "  x: 0.51700359582901\n",
       "  y: 0.3196294605731964\n",
       "  z: -0.006164470221847296\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5203465223312378\n",
       "  y: 0.3171488642692566\n",
       "  z: -0.00695626437664032\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5237703323364258\n",
       "  y: 0.3417567312717438\n",
       "  z: -0.0019346184562891722\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5232421159744263\n",
       "  y: 0.3373025357723236\n",
       "  z: -0.00475730886682868\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5229077935218811\n",
       "  y: 0.3339698016643524\n",
       "  z: -0.005340785253793001\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5225353240966797\n",
       "  y: 0.3306792974472046\n",
       "  z: -0.004824213683605194\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5223065614700317\n",
       "  y: 0.32840293645858765\n",
       "  z: -0.004360627382993698\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5151276588439941\n",
       "  y: 0.3299221396446228\n",
       "  z: -0.002842832589522004\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5147116184234619\n",
       "  y: 0.33103102445602417\n",
       "  z: -0.0032373236026614904\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5141511559486389\n",
       "  y: 0.33303022384643555\n",
       "  z: -0.003659461857751012\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5137360692024231\n",
       "  y: 0.33521419763565063\n",
       "  z: -0.0031782633159309626\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5081006288528442\n",
       "  y: 0.317497193813324\n",
       "  z: -0.005472772754728794\n",
       "}\n",
       "landmark {\n",
       "  x: 0.47702306509017944\n",
       "  y: 0.3211783766746521\n",
       "  z: 0.02421167865395546\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5222848057746887\n",
       "  y: 0.2938278317451477\n",
       "  z: -0.01142874639481306\n",
       "}\n",
       "landmark {\n",
       "  x: 0.512677013874054\n",
       "  y: 0.3294532299041748\n",
       "  z: -0.001767751295119524\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5118650197982788\n",
       "  y: 0.33011293411254883\n",
       "  z: -0.002037817845121026\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5178089737892151\n",
       "  y: 0.2991505563259125\n",
       "  z: -0.009110711514949799\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5117982625961304\n",
       "  y: 0.3020203709602356\n",
       "  z: -0.006705670617520809\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5172174572944641\n",
       "  y: 0.29839441180229187\n",
       "  z: -0.009094657376408577\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5029546022415161\n",
       "  y: 0.28828296065330505\n",
       "  z: -0.004280332941561937\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4984092116355896\n",
       "  y: 0.2954944372177124\n",
       "  z: -0.004373206757009029\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5090969800949097\n",
       "  y: 0.2973516285419464\n",
       "  z: -0.00840272568166256\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4752025604248047\n",
       "  y: 0.2442517727613449\n",
       "  z: 0.0011868973961099982\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4804117679595947\n",
       "  y: 0.2493479996919632\n",
       "  z: -0.0025493167340755463\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4855629801750183\n",
       "  y: 0.2553750276565552\n",
       "  z: -0.0057152449153363705\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5132060050964355\n",
       "  y: 0.3393379747867584\n",
       "  z: -0.001445556292310357\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49989911913871765\n",
       "  y: 0.24678346514701843\n",
       "  z: -0.007467580959200859\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49549055099487305\n",
       "  y: 0.23499195277690887\n",
       "  z: -0.005702765192836523\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4906788468360901\n",
       "  y: 0.22384725511074066\n",
       "  z: -0.00410230690613389\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4900203049182892\n",
       "  y: 0.2846870422363281\n",
       "  z: -0.001507931505329907\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48320797085762024\n",
       "  y: 0.2950265109539032\n",
       "  z: 0.0016277339309453964\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5025215744972229\n",
       "  y: 0.27250564098358154\n",
       "  z: -0.0007806532084941864\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48351478576660156\n",
       "  y: 0.27861306071281433\n",
       "  z: -0.000746941426768899\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5074024200439453\n",
       "  y: 0.27766433358192444\n",
       "  z: -0.005316383205354214\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5128719806671143\n",
       "  y: 0.29246556758880615\n",
       "  z: -0.013531661592423916\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4801219701766968\n",
       "  y: 0.3018511235713959\n",
       "  z: 0.00421973317861557\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48664358258247375\n",
       "  y: 0.29649877548217773\n",
       "  z: -0.0007492949371226132\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4915319085121155\n",
       "  y: 0.29499927163124084\n",
       "  z: -0.0028168964199721813\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4975659251213074\n",
       "  y: 0.2898400127887726\n",
       "  z: -0.003038551192730665\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5015885233879089\n",
       "  y: 0.2845689356327057\n",
       "  z: -0.002879484323784709\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5043390989303589\n",
       "  y: 0.2796649932861328\n",
       "  z: -0.0029188639018684626\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5106213688850403\n",
       "  y: 0.2667461037635803\n",
       "  z: -0.007632729597389698\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4831933379173279\n",
       "  y: 0.31265419721603394\n",
       "  z: 0.0034062254708260298\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48049604892730713\n",
       "  y: 0.2795684039592743\n",
       "  z: 0.00023558150860480964\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5211110711097717\n",
       "  y: 0.2931337356567383\n",
       "  z: -0.015441738069057465\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5075521469116211\n",
       "  y: 0.2871307134628296\n",
       "  z: -0.005996848922222853\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4702838063240051\n",
       "  y: 0.29662200808525085\n",
       "  z: 0.02167668007314205\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5061346888542175\n",
       "  y: 0.2748901844024658\n",
       "  z: -0.002952124457806349\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5082558393478394\n",
       "  y: 0.2984628677368164\n",
       "  z: -0.005323476158082485\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48580995202064514\n",
       "  y: 0.2817972004413605\n",
       "  z: 0.0003059590526390821\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5114200711250305\n",
       "  y: 0.28975847363471985\n",
       "  z: -0.012371673248708248\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48152488470077515\n",
       "  y: 0.3335329294204712\n",
       "  z: 0.023126285523176193\n",
       "}\n",
       "landmark {\n",
       "  x: 0.501876950263977\n",
       "  y: 0.2710501551628113\n",
       "  z: -0.00012029727804474533\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5139477252960205\n",
       "  y: 0.2842152714729309\n",
       "  z: -0.01486512552946806\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5003489851951599\n",
       "  y: 0.3510097861289978\n",
       "  z: 0.00729224132373929\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5011550188064575\n",
       "  y: 0.3582908511161804\n",
       "  z: 0.012073369696736336\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4788459241390228\n",
       "  y: 0.3185342252254486\n",
       "  z: 0.013617484830319881\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49395525455474854\n",
       "  y: 0.34738466143608093\n",
       "  z: 0.009662006981670856\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4728476405143738\n",
       "  y: 0.2822211682796478\n",
       "  z: 0.009465024806559086\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5186699628829956\n",
       "  y: 0.3577161133289337\n",
       "  z: 0.0014484419953078032\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5210703015327454\n",
       "  y: 0.29450273513793945\n",
       "  z: -0.011402697302401066\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5053179264068604\n",
       "  y: 0.29316744208335876\n",
       "  z: -0.005366198252886534\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4780071973800659\n",
       "  y: 0.29154694080352783\n",
       "  z: 0.00542721152305603\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4929829239845276\n",
       "  y: 0.2803202271461487\n",
       "  z: -0.0021504047326743603\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4958495795726776\n",
       "  y: 0.2785932123661041\n",
       "  z: -0.0024574242997914553\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5102593302726746\n",
       "  y: 0.3324007987976074\n",
       "  z: -0.001742224209010601\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4859721064567566\n",
       "  y: 0.3238295316696167\n",
       "  z: 0.004613844677805901\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5256641507148743\n",
       "  y: 0.3606799840927124\n",
       "  z: 0.0029916486237198114\n",
       "}\n",
       "landmark {\n",
       "  x: 0.514108419418335\n",
       "  y: 0.36163073778152466\n",
       "  z: 0.0067286621779203415\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5083922743797302\n",
       "  y: 0.36068740487098694\n",
       "  z: 0.009039514698088169\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5039076805114746\n",
       "  y: 0.22918905317783356\n",
       "  z: -0.004905663430690765\n",
       "}\n",
       "landmark {\n",
       "  x: 0.531804084777832\n",
       "  y: 0.35669633746147156\n",
       "  z: 0.003527976805344224\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4981648623943329\n",
       "  y: 0.27638334035873413\n",
       "  z: -0.002059232909232378\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5001015663146973\n",
       "  y: 0.2737206220626831\n",
       "  z: -0.0012496768031269312\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5012959241867065\n",
       "  y: 0.2719918191432953\n",
       "  z: -0.0003346032463014126\n",
       "}\n",
       "landmark {\n",
       "  x: 0.47720518708229065\n",
       "  y: 0.2803666889667511\n",
       "  z: 0.002750323386862874\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49891144037246704\n",
       "  y: 0.27086517214775085\n",
       "  z: -0.001729902345687151\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4962516725063324\n",
       "  y: 0.2721283435821533\n",
       "  z: -0.0026530425529927015\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4938070476055145\n",
       "  y: 0.2737981975078583\n",
       "  z: -0.0029698682483285666\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49125078320503235\n",
       "  y: 0.2762404680252075\n",
       "  z: -0.0026887510903179646\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48946261405944824\n",
       "  y: 0.27845847606658936\n",
       "  z: -0.0020289276726543903\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4684111177921295\n",
       "  y: 0.28253114223480225\n",
       "  z: 0.017052875831723213\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49079620838165283\n",
       "  y: 0.2811293601989746\n",
       "  z: -0.001501765102148056\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5223228335380554\n",
       "  y: 0.3003709316253662\n",
       "  z: -0.00851554237306118\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5111088156700134\n",
       "  y: 0.3115904927253723\n",
       "  z: -0.0070009916089475155\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5136762261390686\n",
       "  y: 0.2967945337295532\n",
       "  z: -0.010376784950494766\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5178422927856445\n",
       "  y: 0.30447810888290405\n",
       "  z: -0.008912225253880024\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5110743045806885\n",
       "  y: 0.25671976804733276\n",
       "  z: -0.005984563380479813\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5068651437759399\n",
       "  y: 0.35417553782463074\n",
       "  z: 0.005844537168741226\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5125524997711182\n",
       "  y: 0.35611221194267273\n",
       "  z: 0.003881855634972453\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5252561569213867\n",
       "  y: 0.3571437895298004\n",
       "  z: -0.0002906066656578332\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4945516884326935\n",
       "  y: 0.3538261651992798\n",
       "  z: 0.016263317316770554\n",
       "}\n",
       "landmark {\n",
       "  x: 0.500912606716156\n",
       "  y: 0.2707688808441162\n",
       "  z: -0.0007746972260065377\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5108725428581238\n",
       "  y: 0.2762312591075897\n",
       "  z: -0.009020521305501461\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5315707325935364\n",
       "  y: 0.35298413038253784\n",
       "  z: 0.00031811484950594604\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5198401212692261\n",
       "  y: 0.3618795871734619\n",
       "  z: 0.004225254990160465\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4826243817806244\n",
       "  y: 0.3303293287754059\n",
       "  z: 0.013438897207379341\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5185614228248596\n",
       "  y: 0.32957711815834045\n",
       "  z: -0.0038016997277736664\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5183548331260681\n",
       "  y: 0.3314133286476135\n",
       "  z: -0.004242554306983948\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5182303786277771\n",
       "  y: 0.3341655135154724\n",
       "  z: -0.004705807659775019\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5183051824569702\n",
       "  y: 0.3373071551322937\n",
       "  z: -0.004222951829433441\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5180785655975342\n",
       "  y: 0.3416918218135834\n",
       "  z: -0.0021754822228103876\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5108599066734314\n",
       "  y: 0.3253728747367859\n",
       "  z: -0.0032285500783473253\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5098000168800354\n",
       "  y: 0.32544800639152527\n",
       "  z: -0.0033233475405722857\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5087209939956665\n",
       "  y: 0.32522955536842346\n",
       "  z: -0.003479219973087311\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5058417320251465\n",
       "  y: 0.3239750564098358\n",
       "  z: -0.003377720946446061\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49158069491386414\n",
       "  y: 0.322457492351532\n",
       "  z: -0.00045788363786414266\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5089632272720337\n",
       "  y: 0.2718770503997803\n",
       "  z: -0.006276668515056372\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5036649703979492\n",
       "  y: 0.26424217224121094\n",
       "  z: -0.0014091187622398138\n",
       "}\n",
       "landmark {\n",
       "  x: 0.501802384853363\n",
       "  y: 0.2668427526950836\n",
       "  z: -0.0008616425329819322\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5113556981086731\n",
       "  y: 0.32423731684684753\n",
       "  z: -0.002937102923169732\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4939308762550354\n",
       "  y: 0.3371516168117523\n",
       "  z: 0.00435580313205719\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5069694519042969\n",
       "  y: 0.26106885075569153\n",
       "  z: -0.0047904690727591515\n",
       "}\n",
       "landmark {\n",
       "  x: 0.517763078212738\n",
       "  y: 0.34668752551078796\n",
       "  z: -0.0008503868011757731\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5172956585884094\n",
       "  y: 0.27232518792152405\n",
       "  z: -0.013650697655975819\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5128483772277832\n",
       "  y: 0.2713475525379181\n",
       "  z: -0.01058895979076624\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5154266953468323\n",
       "  y: 0.2679639458656311\n",
       "  z: -0.010753130540251732\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5105773210525513\n",
       "  y: 0.2857332229614258\n",
       "  z: -0.00947224348783493\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5307700634002686\n",
       "  y: 0.34817588329315186\n",
       "  z: -0.001090032747015357\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5295858979225159\n",
       "  y: 0.3428919017314911\n",
       "  z: -0.001108285621739924\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5240510106086731\n",
       "  y: 0.34654173254966736\n",
       "  z: -0.0014060548273846507\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5074897408485413\n",
       "  y: 0.3399845361709595\n",
       "  z: 0.0005562498117797077\n",
       "}\n",
       "landmark {\n",
       "  x: 0.506200909614563\n",
       "  y: 0.30522388219833374\n",
       "  z: -0.004796143155544996\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5123372673988342\n",
       "  y: 0.3442899286746979\n",
       "  z: 5.019542368245311e-05\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4984032213687897\n",
       "  y: 0.3111059069633484\n",
       "  z: -0.004812812432646751\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5040199160575867\n",
       "  y: 0.3134804368019104\n",
       "  z: -0.004419548436999321\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49735376238822937\n",
       "  y: 0.3224533796310425\n",
       "  z: -0.0026445137336850166\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5246395468711853\n",
       "  y: 0.3522247076034546\n",
       "  z: -0.0015325439162552357\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5088875889778137\n",
       "  y: 0.2904171943664551\n",
       "  z: -0.007704691030085087\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5063976645469666\n",
       "  y: 0.3466147482395172\n",
       "  z: 0.0027394660282880068\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5120627284049988\n",
       "  y: 0.34980159997940063\n",
       "  z: 0.001912447507493198\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5032263398170471\n",
       "  y: 0.3342365324497223\n",
       "  z: -0.00030970893567427993\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4888820946216583\n",
       "  y: 0.33310467004776\n",
       "  z: 0.006038738880306482\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49979087710380554\n",
       "  y: 0.33935707807540894\n",
       "  z: 0.0018373962957412004\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4874197542667389\n",
       "  y: 0.3402296006679535\n",
       "  z: 0.012553391046822071\n",
       "}\n",
       "landmark {\n",
       "  x: 0.502406895160675\n",
       "  y: 0.3225659132003784\n",
       "  z: -0.0031667982693761587\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5092356204986572\n",
       "  y: 0.28168249130249023\n",
       "  z: -0.007358695846050978\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5145974159240723\n",
       "  y: 0.2939232587814331\n",
       "  z: -0.01405757199972868\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5122718811035156\n",
       "  y: 0.2973448932170868\n",
       "  z: -0.01103261113166809\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5156035423278809\n",
       "  y: 0.28881263732910156\n",
       "  z: -0.015825370326638222\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5002294182777405\n",
       "  y: 0.26070526242256165\n",
       "  z: -0.002731514163315296\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49467533826828003\n",
       "  y: 0.2607174515724182\n",
       "  z: -0.004068049136549234\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49017783999443054\n",
       "  y: 0.2632054090499878\n",
       "  z: -0.0046938457526266575\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48657968640327454\n",
       "  y: 0.2673928141593933\n",
       "  z: -0.004143234342336655\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4842364192008972\n",
       "  y: 0.2725366950035095\n",
       "  z: -0.002728469902649522\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48389285802841187\n",
       "  y: 0.2845940887928009\n",
       "  z: 0.0012681413209065795\n",
       "}\n",
       "landmark {\n",
       "  x: 0.47577375173568726\n",
       "  y: 0.3069649934768677\n",
       "  z: 0.012853887863457203\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48844367265701294\n",
       "  y: 0.28972557187080383\n",
       "  z: -0.0007790194940753281\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4923805594444275\n",
       "  y: 0.28857672214508057\n",
       "  z: -0.0019086161628365517\n",
       "}\n",
       "landmark {\n",
       "  x: 0.49668923020362854\n",
       "  y: 0.28554993867874146\n",
       "  z: -0.0023774681612849236\n",
       "}\n",
       "landmark {\n",
       "  x: 0.500352680683136\n",
       "  y: 0.28144368529319763\n",
       "  z: -0.002321315463632345\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5028512477874756\n",
       "  y: 0.27731168270111084\n",
       "  z: -0.001623496413230896\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5044727921485901\n",
       "  y: 0.273875892162323\n",
       "  z: -0.0015055563999339938\n",
       "}\n",
       "landmark {\n",
       "  x: 0.47346019744873047\n",
       "  y: 0.30891725420951843\n",
       "  z: 0.023835452273488045\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5122600197792053\n",
       "  y: 0.29893097281455994\n",
       "  z: -0.009514405392110348\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5126016139984131\n",
       "  y: 0.2799454629421234\n",
       "  z: -0.011556786485016346\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5174190998077393\n",
       "  y: 0.291938841342926\n",
       "  z: -0.015887737274169922\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5191156268119812\n",
       "  y: 0.29392731189727783\n",
       "  z: -0.014375166036188602\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5171980857849121\n",
       "  y: 0.293305903673172\n",
       "  z: -0.014487089589238167\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5130320191383362\n",
       "  y: 0.30018383264541626\n",
       "  z: -0.00826051365584135\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5199955105781555\n",
       "  y: 0.2936595380306244\n",
       "  z: -0.015107134357094765\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5199972987174988\n",
       "  y: 0.29507187008857727\n",
       "  z: -0.011532035656273365\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5031424760818481\n",
       "  y: 0.26990216970443726\n",
       "  z: -0.00019319656712468714\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5053524374961853\n",
       "  y: 0.26956433057785034\n",
       "  z: -0.0015403373399749398\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5070075988769531\n",
       "  y: 0.26922300457954407\n",
       "  z: -0.0034028603695333004\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4883037507534027\n",
       "  y: 0.2800023853778839\n",
       "  z: -0.0012153516290709376\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48546943068504333\n",
       "  y: 0.2778681814670563\n",
       "  z: -0.0012083664769306779\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5196865200996399\n",
       "  y: 0.27151259779930115\n",
       "  z: -0.012028140015900135\n",
       "}\n",
       "landmark {\n",
       "  x: 0.530394971370697\n",
       "  y: 0.25023216009140015\n",
       "  z: 0.008048293180763721\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5243615508079529\n",
       "  y: 0.2909483313560486\n",
       "  z: -0.01081970613449812\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5314797163009644\n",
       "  y: 0.22459937632083893\n",
       "  z: 0.02497546374797821\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5238856077194214\n",
       "  y: 0.26054847240448\n",
       "  z: 0.0033029953483492136\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5266706347465515\n",
       "  y: 0.25896769762039185\n",
       "  z: 0.004167862702161074\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5291141867637634\n",
       "  y: 0.25659650564193726\n",
       "  z: 0.00559720303863287\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5315698981285095\n",
       "  y: 0.2509866952896118\n",
       "  z: 0.008847374469041824\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5213666558265686\n",
       "  y: 0.26064738631248474\n",
       "  z: 0.0030820677056908607\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5240340828895569\n",
       "  y: 0.2420630156993866\n",
       "  z: 0.003019392490386963\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5212197303771973\n",
       "  y: 0.24506638944149017\n",
       "  z: 0.0022981835063546896\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5269961953163147\n",
       "  y: 0.24063903093338013\n",
       "  z: 0.004666587337851524\n",
       "}\n",
       "landmark {\n",
       "  x: 0.529222846031189\n",
       "  y: 0.24160221219062805\n",
       "  z: 0.006238729227334261\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5340738892555237\n",
       "  y: 0.2529151141643524\n",
       "  z: 0.01072090957313776\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5383848547935486\n",
       "  y: 0.3346444070339203\n",
       "  z: 0.004296010360121727\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5308703780174255\n",
       "  y: 0.24823294579982758\n",
       "  z: 0.009283455088734627\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5375161170959473\n",
       "  y: 0.24697065353393555\n",
       "  z: 0.026066860184073448\n",
       "}\n",
       "landmark {\n",
       "  x: 0.535040020942688\n",
       "  y: 0.24748410284519196\n",
       "  z: 0.014435232616961002\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5319939851760864\n",
       "  y: 0.2781383991241455\n",
       "  z: 0.000508460565470159\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5273023843765259\n",
       "  y: 0.3049613833427429\n",
       "  z: -0.00791147816926241\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5274047255516052\n",
       "  y: 0.31113117933273315\n",
       "  z: -0.006610106211155653\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5308475494384766\n",
       "  y: 0.30413663387298584\n",
       "  z: -0.005070962943136692\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5329039096832275\n",
       "  y: 0.30497652292251587\n",
       "  z: -0.001542797195725143\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5298168659210205\n",
       "  y: 0.3097575306892395\n",
       "  z: -0.004367506597191095\n",
       "}\n",
       "landmark {\n",
       "  x: 0.531453549861908\n",
       "  y: 0.30931222438812256\n",
       "  z: -0.0012870719656348228\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5370750427246094\n",
       "  y: 0.31417104601860046\n",
       "  z: 0.0051252832636237144\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5246326327323914\n",
       "  y: 0.2870404124259949\n",
       "  z: -0.016178153455257416\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5238423943519592\n",
       "  y: 0.2815503478050232\n",
       "  z: -0.016969844698905945\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5310612320899963\n",
       "  y: 0.23264941573143005\n",
       "  z: 0.008113431744277477\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5234120488166809\n",
       "  y: 0.2696651816368103\n",
       "  z: -0.0008828387362882495\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5287556052207947\n",
       "  y: 0.28282633423805237\n",
       "  z: -0.007493737619370222\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5281168222427368\n",
       "  y: 0.2802972197532654\n",
       "  z: -0.006499729119241238\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5390012860298157\n",
       "  y: 0.27230775356292725\n",
       "  z: 0.00644295709207654\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5217835307121277\n",
       "  y: 0.2758081555366516\n",
       "  z: -0.01500572357326746\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5243929624557495\n",
       "  y: 0.2304759919643402\n",
       "  z: 0.0014480119571089745\n",
       "}\n",
       "landmark {\n",
       "  x: 0.528120756149292\n",
       "  y: 0.22989889979362488\n",
       "  z: 0.00482529541477561\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5277162790298462\n",
       "  y: 0.2159891426563263\n",
       "  z: 0.018389718607068062\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5146739482879639\n",
       "  y: 0.24410030245780945\n",
       "  z: -0.0038589942269027233\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5192158818244934\n",
       "  y: 0.24919669330120087\n",
       "  z: 0.002557592000812292\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5375300645828247\n",
       "  y: 0.3061656951904297\n",
       "  z: 0.005900959484279156\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5436499714851379\n",
       "  y: 0.30028870701789856\n",
       "  z: 0.0331612266600132\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5273441076278687\n",
       "  y: 0.28741776943206787\n",
       "  z: -0.006901073735207319\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5256561636924744\n",
       "  y: 0.2908427119255066\n",
       "  z: -0.007703687530010939\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5352327823638916\n",
       "  y: 0.3085434138774872\n",
       "  z: 0.005202917847782373\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5338420867919922\n",
       "  y: 0.3097745180130005\n",
       "  z: 0.0040243095718324184\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5283390879631042\n",
       "  y: 0.2254304140806198\n",
       "  z: 0.00651496322825551\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5289941430091858\n",
       "  y: 0.2854141592979431\n",
       "  z: -0.005888138897716999\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5199779272079468\n",
       "  y: 0.23470164835453033\n",
       "  z: -0.0015482916496694088\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5194244980812073\n",
       "  y: 0.22988176345825195\n",
       "  z: -0.002107800915837288\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5160751938819885\n",
       "  y: 0.20891954004764557\n",
       "  z: 0.004150785505771637\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5280699729919434\n",
       "  y: 0.22065386176109314\n",
       "  z: 0.011654151603579521\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5175235271453857\n",
       "  y: 0.21922916173934937\n",
       "  z: 0.0009104039054363966\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5315330624580383\n",
       "  y: 0.2294701486825943\n",
       "  z: 0.010980229824781418\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5317143201828003\n",
       "  y: 0.22733278572559357\n",
       "  z: 0.01753103733062744\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5275408625602722\n",
       "  y: 0.3083588778972626\n",
       "  z: -0.007562193088233471\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5303861498832703\n",
       "  y: 0.30737000703811646\n",
       "  z: -0.004817884881049395\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5322383046150208\n",
       "  y: 0.30710119009017944\n",
       "  z: -0.00166696694213897\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5268259048461914\n",
       "  y: 0.28903481364250183\n",
       "  z: -0.006376186385750771\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5346012115478516\n",
       "  y: 0.30916735529899597\n",
       "  z: 0.004555158317089081\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5344820022583008\n",
       "  y: 0.3135301470756531\n",
       "  z: 0.002817156258970499\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5332337021827698\n",
       "  y: 0.3100322484970093\n",
       "  z: 0.003942668903619051\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5264089107513428\n",
       "  y: 0.2859683632850647\n",
       "  z: -0.011051544919610023\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5305516123771667\n",
       "  y: 0.3097231984138489\n",
       "  z: -0.0009568247478455305\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5289862751960754\n",
       "  y: 0.3106935918331146\n",
       "  z: -0.0033573093824088573\n",
       "}\n",
       "landmark {\n",
       "  x: 0.526867687702179\n",
       "  y: 0.31236642599105835\n",
       "  z: -0.005362464115023613\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5326005816459656\n",
       "  y: 0.3347189128398895\n",
       "  z: -0.00033602066105231643\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5320167541503906\n",
       "  y: 0.330539345741272\n",
       "  z: -0.002978066448122263\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5312093496322632\n",
       "  y: 0.327504962682724\n",
       "  z: -0.0035056667402386665\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5301850438117981\n",
       "  y: 0.32476335763931274\n",
       "  z: -0.00298779783770442\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5292173624038696\n",
       "  y: 0.3230430483818054\n",
       "  z: -0.002603615168482065\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5322679281234741\n",
       "  y: 0.3169054090976715\n",
       "  z: 0.0009929336374625564\n",
       "}\n",
       "landmark {\n",
       "  x: 0.533070981502533\n",
       "  y: 0.3171110153198242\n",
       "  z: 0.0005920155672356486\n",
       "}\n",
       "landmark {\n",
       "  x: 0.534216582775116\n",
       "  y: 0.31776073575019836\n",
       "  z: 0.00033144745975732803\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5351647138595581\n",
       "  y: 0.3187771439552307\n",
       "  z: 0.0010427054949104786\n",
       "}\n",
       "landmark {\n",
       "  x: 0.534004271030426\n",
       "  y: 0.29794204235076904\n",
       "  z: -0.00038434204179793596\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5404122471809387\n",
       "  y: 0.2724108099937439\n",
       "  z: 0.03847656026482582\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5327755212783813\n",
       "  y: 0.31441858410835266\n",
       "  z: 0.002890183124691248\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5336554050445557\n",
       "  y: 0.31380027532577515\n",
       "  z: 0.0026847419794648886\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5252749919891357\n",
       "  y: 0.2935436964035034\n",
       "  z: -0.007846727967262268\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5285984873771667\n",
       "  y: 0.28948280215263367\n",
       "  z: -0.00399803975597024\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5255031585693359\n",
       "  y: 0.29225993156433105\n",
       "  z: -0.007856635376811028\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5268246531486511\n",
       "  y: 0.27048948407173157\n",
       "  z: 0.0008092131465673447\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5319226384162903\n",
       "  y: 0.27055567502975464\n",
       "  z: 0.002689547371119261\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5287855863571167\n",
       "  y: 0.282609760761261\n",
       "  z: -0.004694269970059395\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5227598547935486\n",
       "  y: 0.21044212579727173\n",
       "  z: 0.011071620509028435\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5235065221786499\n",
       "  y: 0.21748429536819458\n",
       "  z: 0.006124849431216717\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5242085456848145\n",
       "  y: 0.22560849785804749\n",
       "  z: 0.002178937429562211\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5364266633987427\n",
       "  y: 0.3210221827030182\n",
       "  z: 0.0034054305870085955\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5138607621192932\n",
       "  y: 0.23613400757312775\n",
       "  z: -0.004822484217584133\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5113403797149658\n",
       "  y: 0.2235085517168045\n",
       "  z: -0.0027149615343660116\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5089017748832703\n",
       "  y: 0.21109363436698914\n",
       "  z: -0.0006775592337362468\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5309675931930542\n",
       "  y: 0.25389236211776733\n",
       "  z: 0.007376018445938826\n",
       "}\n",
       "landmark {\n",
       "  x: 0.537156879901886\n",
       "  y: 0.2546235918998718\n",
       "  z: 0.013014938682317734\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5196533799171448\n",
       "  y: 0.2600409686565399\n",
       "  z: 0.0031945141963660717\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5317978262901306\n",
       "  y: 0.24179035425186157\n",
       "  z: 0.009519149549305439\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5208647847175598\n",
       "  y: 0.26757681369781494\n",
       "  z: -0.0023522686678916216\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5273376107215881\n",
       "  y: 0.28147006034851074\n",
       "  z: -0.010775160975754261\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5399491786956787\n",
       "  y: 0.2567051351070404\n",
       "  z: 0.01709725335240364\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5369824171066284\n",
       "  y: 0.25890958309173584\n",
       "  z: 0.009909841232001781\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5347049236297607\n",
       "  y: 0.2628602087497711\n",
       "  z: 0.006348022259771824\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5297465324401855\n",
       "  y: 0.26592782139778137\n",
       "  z: 0.003991854842752218\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5255489945411682\n",
       "  y: 0.2667660117149353\n",
       "  z: 0.00239666854031384\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5222312211990356\n",
       "  y: 0.2663876414299011\n",
       "  z: 0.0009610092965885997\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5161699056625366\n",
       "  y: 0.26259347796440125\n",
       "  z: -0.006348366383463144\n",
       "}\n",
       "landmark {\n",
       "  x: 0.542718231678009\n",
       "  y: 0.26738107204437256\n",
       "  z: 0.01625935547053814\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5330671668052673\n",
       "  y: 0.23963944613933563\n",
       "  z: 0.011413452215492725\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5239212512969971\n",
       "  y: 0.29101723432540894\n",
       "  z: -0.014848347753286362\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5247973203659058\n",
       "  y: 0.27423325181007385\n",
       "  z: -0.0024486705660820007\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5363012552261353\n",
       "  y: 0.2477039247751236\n",
       "  z: 0.03648022934794426\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5196400284767151\n",
       "  y: 0.2648761570453644\n",
       "  z: 5.5482960306108e-05\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5287678241729736\n",
       "  y: 0.2831711769104004\n",
       "  z: -0.0015987533843144774\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5316079258918762\n",
       "  y: 0.24715042114257812\n",
       "  z: 0.010122540406882763\n",
       "}\n",
       "landmark {\n",
       "  x: 0.526387095451355\n",
       "  y: 0.2784297466278076\n",
       "  z: -0.009586801752448082\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5424163937568665\n",
       "  y: 0.2856874167919159\n",
       "  z: 0.03665297105908394\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5191493034362793\n",
       "  y: 0.25832894444465637\n",
       "  z: 0.00401886273175478\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5240720510482788\n",
       "  y: 0.2764647305011749\n",
       "  z: -0.01283663883805275\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5436763763427734\n",
       "  y: 0.31460800766944885\n",
       "  z: 0.01670115999877453\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5435280203819275\n",
       "  y: 0.32165616750717163\n",
       "  z: 0.021531177684664726\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5430353283882141\n",
       "  y: 0.2691904902458191\n",
       "  z: 0.027476750314235687\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5449663996696472\n",
       "  y: 0.30557745695114136\n",
       "  z: 0.02075943350791931\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5345658659934998\n",
       "  y: 0.23653988540172577\n",
       "  z: 0.022754833102226257\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5393859148025513\n",
       "  y: 0.33922696113586426\n",
       "  z: 0.005961514078080654\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5233274102210999\n",
       "  y: 0.2928932011127472\n",
       "  z: -0.011052504181861877\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5278787016868591\n",
       "  y: 0.2763625383377075\n",
       "  z: -0.000913373485673219\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5366648435592651\n",
       "  y: 0.2473563402891159\n",
       "  z: 0.01802104339003563\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5283313989639282\n",
       "  y: 0.253905713558197\n",
       "  z: 0.005687763914465904\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5260679125785828\n",
       "  y: 0.2561647891998291\n",
       "  z: 0.004517850931733847\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5352848172187805\n",
       "  y: 0.3135331869125366\n",
       "  z: 0.003410839010030031\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5444201231002808\n",
       "  y: 0.27864155173301697\n",
       "  z: 0.016996722668409348\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5366949439048767\n",
       "  y: 0.3506353497505188\n",
       "  z: 0.005582152400165796\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5408830642700195\n",
       "  y: 0.337344229221344\n",
       "  z: 0.012906373478472233\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5420745611190796\n",
       "  y: 0.33055368065834045\n",
       "  z: 0.016751695424318314\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5236917734146118\n",
       "  y: 0.2575882375240326\n",
       "  z: 0.003835968906059861\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5213747024536133\n",
       "  y: 0.25816401839256287\n",
       "  z: 0.003799260128289461\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5199143886566162\n",
       "  y: 0.2583637535572052\n",
       "  z: 0.004172659479081631\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5339584946632385\n",
       "  y: 0.23760643601417542\n",
       "  z: 0.014933786354959011\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5205895900726318\n",
       "  y: 0.2545751631259918\n",
       "  z: 0.003332361113280058\n",
       "}\n",
       "landmark {\n",
       "  x: 0.522580087184906\n",
       "  y: 0.2521700859069824\n",
       "  z: 0.0032799895852804184\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5247844457626343\n",
       "  y: 0.2500758469104767\n",
       "  z: 0.003954862244427204\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5272741317749023\n",
       "  y: 0.24864605069160461\n",
       "  z: 0.005233318079262972\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5290007591247559\n",
       "  y: 0.2483232021331787\n",
       "  z: 0.00650601414963603\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5341856479644775\n",
       "  y: 0.23463007807731628\n",
       "  z: 0.0315067321062088\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5296436548233032\n",
       "  y: 0.251910537481308\n",
       "  z: 0.006990624591708183\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5314571261405945\n",
       "  y: 0.2961958646774292\n",
       "  z: -0.003205898217856884\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5274574756622314\n",
       "  y: 0.28631120920181274\n",
       "  z: -0.007976875640451908\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5262836813926697\n",
       "  y: 0.29804086685180664\n",
       "  z: -0.007327591069042683\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5421996116638184\n",
       "  y: 0.32364484667778015\n",
       "  z: 0.013422942720353603\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5406083464622498\n",
       "  y: 0.3311519920825958\n",
       "  z: 0.009906604886054993\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5366541147232056\n",
       "  y: 0.3470194339752197\n",
       "  z: 0.0022265128791332245\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5440375208854675\n",
       "  y: 0.31249570846557617\n",
       "  z: 0.02756921574473381\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5195643305778503\n",
       "  y: 0.2568695545196533\n",
       "  z: 0.0037540949415415525\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5202746391296387\n",
       "  y: 0.26910701394081116\n",
       "  z: -0.007039377465844154\n",
       "}\n",
       "landmark {\n",
       "  x: 0.539352297782898\n",
       "  y: 0.3440168499946594\n",
       "  z: 0.008801872842013836\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5447036623954773\n",
       "  y: 0.2816586196422577\n",
       "  z: 0.02668720670044422\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5311297178268433\n",
       "  y: 0.31994399428367615\n",
       "  z: -0.0008806598489172757\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5321063995361328\n",
       "  y: 0.32089418172836304\n",
       "  z: -0.001204981468617916\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5332616567611694\n",
       "  y: 0.32264113426208496\n",
       "  z: -0.001513547613285482\n",
       "}\n",
       "landmark {\n",
       "  x: 0.534252405166626\n",
       "  y: 0.3249894380569458\n",
       "  z: -0.0008659938466735184\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5352476239204407\n",
       "  y: 0.3280520737171173\n",
       "  z: 0.0014917253283783793\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5327999591827393\n",
       "  y: 0.3090870678424835\n",
       "  z: 0.001628455356694758\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5336798429489136\n",
       "  y: 0.3076424300670624\n",
       "  z: 0.001902619726024568\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5343075394630432\n",
       "  y: 0.3061007261276245\n",
       "  z: 0.002014159457758069\n",
       "}\n",
       "landmark {\n",
       "  x: 0.536074161529541\n",
       "  y: 0.3011600971221924\n",
       "  z: 0.002984855091199279\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5426666736602783\n",
       "  y: 0.28313711285591125\n",
       "  z: 0.01019660010933876\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5183945894241333\n",
       "  y: 0.2647972106933594\n",
       "  z: -0.0041577923111617565\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5164557695388794\n",
       "  y: 0.25471439957618713\n",
       "  z: 0.0011250983225181699\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5179372429847717\n",
       "  y: 0.2549087107181549\n",
       "  z: 0.0029261461459100246\n",
       "}\n",
       "landmark {\n",
       "  x: 0.531794548034668\n",
       "  y: 0.3092005252838135\n",
       "  z: 0.0017611137591302395\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5440834760665894\n",
       "  y: 0.2972661852836609\n",
       "  z: 0.014922901056706905\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5146103501319885\n",
       "  y: 0.2552821934223175\n",
       "  z: -0.0032585710287094116\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5366315245628357\n",
       "  y: 0.33089351654052734\n",
       "  z: 0.003020284930244088\n",
       "}\n",
       "landmark {\n",
       "  x: 0.518047034740448\n",
       "  y: 0.26738181710243225\n",
       "  z: -0.009521962143480778\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5241888165473938\n",
       "  y: 0.2754112184047699\n",
       "  z: -0.006904252339154482\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5339481234550476\n",
       "  y: 0.3382379710674286\n",
       "  z: 0.0004534257750492543\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5390661358833313\n",
       "  y: 0.3146887421607971\n",
       "  z: 0.007175934035331011\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5316308736801147\n",
       "  y: 0.2861342132091522\n",
       "  z: -7.058926712488756e-06\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5381003022193909\n",
       "  y: 0.3231392502784729\n",
       "  z: 0.005304783117026091\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5372024178504944\n",
       "  y: 0.28171008825302124\n",
       "  z: 0.002849941374734044\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5349245071411133\n",
       "  y: 0.29010438919067383\n",
       "  z: 0.0015815686201676726\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5403881669044495\n",
       "  y: 0.2893350422382355\n",
       "  z: 0.006128209177404642\n",
       "}\n",
       "landmark {\n",
       "  x: 0.535712718963623\n",
       "  y: 0.34264039993286133\n",
       "  z: 0.0008175669354386628\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5260481238365173\n",
       "  y: 0.2775672674179077\n",
       "  z: -0.0044583166018128395\n",
       "}\n",
       "landmark {\n",
       "  x: 0.54120934009552\n",
       "  y: 0.31768158078193665\n",
       "  z: 0.010121018625795841\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5396060347557068\n",
       "  y: 0.3262712359428406\n",
       "  z: 0.007590645924210548\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5397030115127563\n",
       "  y: 0.305641770362854\n",
       "  z: 0.007638772949576378\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5449818968772888\n",
       "  y: 0.2889547049999237\n",
       "  z: 0.017876794561743736\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5424289703369141\n",
       "  y: 0.3051224648952484\n",
       "  z: 0.010854128748178482\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5453673005104065\n",
       "  y: 0.29382455348968506\n",
       "  z: 0.02510034292936325\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5376303195953369\n",
       "  y: 0.2958531975746155\n",
       "  z: 0.004100990481674671\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5224720239639282\n",
       "  y: 0.2717185616493225\n",
       "  z: -0.004569679964333773\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5271174907684326\n",
       "  y: 0.2843630313873291\n",
       "  z: -0.011540427803993225\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5284123420715332\n",
       "  y: 0.2850722074508667\n",
       "  z: -0.00812540017068386\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5257348418235779\n",
       "  y: 0.2810647785663605\n",
       "  z: -0.01384178176522255\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5173340439796448\n",
       "  y: 0.24772150814533234\n",
       "  z: 0.0007357122958637774\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5207290649414062\n",
       "  y: 0.24086523056030273\n",
       "  z: 0.0012926730560138822\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5243061184883118\n",
       "  y: 0.2372128814458847\n",
       "  z: 0.0024652103893458843\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5275998115539551\n",
       "  y: 0.23610468208789825\n",
       "  z: 0.004526983015239239\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5301165580749512\n",
       "  y: 0.2374819815158844\n",
       "  z: 0.006907510571181774\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5330873727798462\n",
       "  y: 0.24740305542945862\n",
       "  z: 0.011730166152119637\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5405781865119934\n",
       "  y: 0.2578471601009369\n",
       "  z: 0.027134962379932404\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5336131453514099\n",
       "  y: 0.2558877170085907\n",
       "  z: 0.008861097507178783\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5315009951591492\n",
       "  y: 0.25933825969696045\n",
       "  z: 0.0064876144751906395\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5281504988670349\n",
       "  y: 0.2621239721775055\n",
       "  z: 0.004552838858217001\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5245351791381836\n",
       "  y: 0.26355400681495667\n",
       "  z: 0.0030728245619684458\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5215508341789246\n",
       "  y: 0.2635226249694824\n",
       "  z: 0.002422070363536477\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5194473266601562\n",
       "  y: 0.262907475233078\n",
       "  z: 0.001769924652762711\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5381324291229248\n",
       "  y: 0.26015418767929077\n",
       "  z: 0.03864022716879845\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5283383131027222\n",
       "  y: 0.2867894768714905\n",
       "  z: -0.006664819549769163\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5220100283622742\n",
       "  y: 0.2727690041065216\n",
       "  z: -0.00954636000096798\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5259121060371399\n",
       "  y: 0.2853846549987793\n",
       "  z: -0.014299783855676651\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5247014760971069\n",
       "  y: 0.289627343416214\n",
       "  z: -0.013463979586958885\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5257515907287598\n",
       "  y: 0.28670328855514526\n",
       "  z: -0.012858167290687561\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5278270244598389\n",
       "  y: 0.289107084274292\n",
       "  z: -0.005839838646352291\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5245355367660522\n",
       "  y: 0.2902355194091797\n",
       "  z: -0.014354335144162178\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5240323543548584\n",
       "  y: 0.29206740856170654\n",
       "  z: -0.010901578702032566\n",
       "}\n",
       "landmark {\n",
       "  x: 0.518279492855072\n",
       "  y: 0.25885409116744995\n",
       "  z: 0.003207033732905984\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5174435973167419\n",
       "  y: 0.26074662804603577\n",
       "  z: 0.000989129999652505\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5171993374824524\n",
       "  y: 0.261707067489624\n",
       "  z: -0.0012555252760648727\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5300794839859009\n",
       "  y: 0.24826012551784515\n",
       "  z: 0.0077447956427931786\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5308533906936646\n",
       "  y: 0.24339911341667175\n",
       "  z: 0.008229058235883713\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5183186531066895"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "landmark {\n",
       "  x: 0.5183186531066895\n",
       "  y: 0.2888590097427368\n",
       "  z: -0.431506872177124\n",
       "  visibility: 0.9999986886978149\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5230835676193237\n",
       "  y: 0.26147329807281494\n",
       "  z: -0.4057560861110687\n",
       "  visibility: 0.9999979734420776\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5283770561218262\n",
       "  y: 0.25966089963912964\n",
       "  z: -0.40576207637786865\n",
       "  visibility: 0.999998927116394\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5332298874855042\n",
       "  y: 0.2575591802597046\n",
       "  z: -0.4057518541812897\n",
       "  visibility: 0.9999978542327881\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5042418241500854\n",
       "  y: 0.26721012592315674\n",
       "  z: -0.4060806930065155\n",
       "  visibility: 0.9999973773956299\n",
       "}\n",
       "landmark {\n",
       "  x: 0.497038871049881\n",
       "  y: 0.27003908157348633\n",
       "  z: -0.40612009167671204\n",
       "  visibility: 0.9999980926513672\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4911555051803589\n",
       "  y: 0.27277565002441406\n",
       "  z: -0.4062861204147339\n",
       "  visibility: 0.999996542930603\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5402431488037109\n",
       "  y: 0.26838093996047974\n",
       "  z: -0.23066753149032593\n",
       "  visibility: 0.9999990463256836\n",
       "}\n",
       "landmark {\n",
       "  x: 0.48231562972068787\n",
       "  y: 0.29092347621917725\n",
       "  z: -0.23012810945510864\n",
       "  visibility: 0.9999990463256836\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5302038788795471\n",
       "  y: 0.31656646728515625\n",
       "  z: -0.3641927242279053\n",
       "  visibility: 0.9999988079071045\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5089635252952576\n",
       "  y: 0.31899765133857727\n",
       "  z: -0.3642071783542633\n",
       "  visibility: 0.9999984502792358\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5847214460372925\n",
       "  y: 0.44406208395957947\n",
       "  z: -0.09096827358007431\n",
       "  visibility: 0.999992847442627\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4407132565975189\n",
       "  y: 0.43550026416778564\n",
       "  z: -0.123698391020298\n",
       "  visibility: 0.999975323677063\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5856548547744751\n",
       "  y: 0.6758774518966675\n",
       "  z: -0.1395939737558365\n",
       "  visibility: 0.9999313354492188\n",
       "}\n",
       "landmark {\n",
       "  x: 0.3905217945575714\n",
       "  y: 0.6330769062042236\n",
       "  z: -0.21758787333965302\n",
       "  visibility: 0.999823272228241\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5756012201309204\n",
       "  y: 0.6850455403327942\n",
       "  z: -0.421399861574173\n",
       "  visibility: 0.9999184608459473\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4363649785518646\n",
       "  y: 0.6737774014472961\n",
       "  z: -0.5269014239311218\n",
       "  visibility: 0.9997099041938782\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5656508207321167\n",
       "  y: 0.6832118034362793\n",
       "  z: -0.484672874212265\n",
       "  visibility: 0.9997227787971497\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4592670500278473\n",
       "  y: 0.7031726241111755\n",
       "  z: -0.5867488980293274\n",
       "  visibility: 0.998519241809845\n",
       "}\n",
       "landmark {\n",
       "  x: 0.563776433467865\n",
       "  y: 0.6575878858566284\n",
       "  z: -0.49237099289894104\n",
       "  visibility: 0.9997370839118958\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4693394601345062\n",
       "  y: 0.6869029998779297\n",
       "  z: -0.5963891744613647\n",
       "  visibility: 0.9987266659736633\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5641582012176514\n",
       "  y: 0.6556602120399475\n",
       "  z: -0.4290895164012909\n",
       "  visibility: 0.9997510313987732\n",
       "}\n",
       "landmark {\n",
       "  x: 0.46467986702919006\n",
       "  y: 0.6771189570426941\n",
       "  z: -0.5346239805221558\n",
       "  visibility: 0.999032735824585\n",
       "}\n",
       "landmark {\n",
       "  x: 0.528692364692688\n",
       "  y: 0.7931528687477112\n",
       "  z: 0.020124370232224464\n",
       "  visibility: 0.9999802112579346\n",
       "}\n",
       "landmark {\n",
       "  x: 0.44348597526550293\n",
       "  y: 0.7830679416656494\n",
       "  z: -0.01899600774049759\n",
       "  visibility: 0.9999744892120361\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5279448628425598\n",
       "  y: 1.0785456895828247\n",
       "  z: 0.006495811510831118\n",
       "  visibility: 0.7583569884300232\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4442884027957916\n",
       "  y: 1.0620307922363281\n",
       "  z: -0.035968538373708725\n",
       "  visibility: 0.5227707028388977\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5216016173362732\n",
       "  y: 1.3119733333587646\n",
       "  z: 0.35407790541648865\n",
       "  visibility: 0.00027400601538829505\n",
       "}\n",
       "landmark {\n",
       "  x: 0.43551015853881836\n",
       "  y: 1.3051015138626099\n",
       "  z: 0.3302070200443268\n",
       "  visibility: 0.00012930059165228158\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5259267091751099\n",
       "  y: 1.3509355783462524\n",
       "  z: 0.38210609555244446\n",
       "  visibility: 0.001392287202179432\n",
       "}\n",
       "landmark {\n",
       "  x: 0.4322648346424103\n",
       "  y: 1.347449541091919\n",
       "  z: 0.36175456643104553\n",
       "  visibility: 0.00041079800575971603\n",
       "}\n",
       "landmark {\n",
       "  x: 0.5043538808822632\n",
       "  y: 1.395870327949524\n",
       "  z: 0.22642064094543457\n",
       "  visibility: 0.00017480766109656543\n",
       "}\n",
       "landmark {\n",
       "  x: 0.44384685158729553\n",
       "  y: 1.3754401206970215\n",
       "  z: 0.18672555685043335\n",
       "  visibility: 0.00018053922394756228\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 167.2ms\n",
      "Speed: 11.0ms preprocess, 167.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 182.0ms\n",
      "Speed: 0.0ms preprocess, 182.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 174.9ms\n",
      "Speed: 3.0ms preprocess, 174.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 207.0ms\n",
      "Speed: 7.5ms preprocess, 207.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 204.8ms\n",
      "Speed: 6.8ms preprocess, 204.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 187.4ms\n",
      "Speed: 10.1ms preprocess, 187.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_240.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 195.9ms\n",
      "Speed: 4.4ms preprocess, 195.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_270.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 188.7ms\n",
      "Speed: 2.1ms preprocess, 188.7ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 283.2ms\n",
      "Speed: 0.0ms preprocess, 283.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_300.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 152.4ms\n",
      "Speed: 3.5ms preprocess, 152.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_330.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 209.0ms\n",
      "Speed: 4.8ms preprocess, 209.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_360.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 191.3ms\n",
      "Speed: 1.4ms preprocess, 191.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_390.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 215.4ms\n",
      "Speed: 5.7ms preprocess, 215.4ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_420.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 141.8ms\n",
      "Speed: 7.5ms preprocess, 141.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_450.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 202.7ms\n",
      "Speed: 5.7ms preprocess, 202.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_480.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 169.3ms\n",
      "Speed: 4.0ms preprocess, 169.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_510.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 154.8ms\n",
      "Speed: 8.0ms preprocess, 154.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_540.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 120.6ms\n",
      "Speed: 7.2ms preprocess, 120.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_570.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 154.4ms\n",
      "Speed: 15.6ms preprocess, 154.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 150.9ms\n",
      "Speed: 0.0ms preprocess, 150.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 1frame_90.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 152.6ms\n",
      "Speed: 3.5ms preprocess, 152.6ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 140.3ms\n",
      "Speed: 0.0ms preprocess, 140.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 160.0ms\n",
      "Speed: 0.0ms preprocess, 160.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 124.5ms\n",
      "Speed: 4.2ms preprocess, 124.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 165.6ms\n",
      "Speed: 0.0ms preprocess, 165.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 140.3ms\n",
      "Speed: 6.6ms preprocess, 140.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 189.7ms\n",
      "Speed: 0.0ms preprocess, 189.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 218.1ms\n",
      "Speed: 0.0ms preprocess, 218.1ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 2frame_90.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 172.6ms\n",
      "Speed: 6.7ms preprocess, 172.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 142.2ms\n",
      "Speed: 1.7ms preprocess, 142.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 154.7ms\n",
      "Speed: 7.5ms preprocess, 154.7ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 147.7ms\n",
      "Speed: 0.0ms preprocess, 147.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 134.9ms\n",
      "Speed: 12.0ms preprocess, 134.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 169.2ms\n",
      "Speed: 0.0ms preprocess, 169.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_240.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 192.9ms\n",
      "Speed: 16.6ms preprocess, 192.9ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 262.3ms\n",
      "Speed: 0.0ms preprocess, 262.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 174.6ms\n",
      "Speed: 16.1ms preprocess, 174.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 32frame_90.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 187.0ms\n",
      "Speed: 0.0ms preprocess, 187.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 179.5ms\n",
      "Speed: 0.0ms preprocess, 179.5ms inference, 9.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 192.5ms\n",
      "Speed: 0.0ms preprocess, 192.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 153.0ms\n",
      "Speed: 3.8ms preprocess, 153.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 163.3ms\n",
      "Speed: 0.0ms preprocess, 163.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 173.7ms\n",
      "Speed: 7.6ms preprocess, 173.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_240.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 183.7ms\n",
      "Speed: 5.9ms preprocess, 183.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_270.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 193.5ms\n",
      "Speed: 8.0ms preprocess, 193.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 190.6ms\n",
      "Speed: 9.2ms preprocess, 190.6ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_300.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 174.7ms\n",
      "Speed: 7.7ms preprocess, 174.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 182.2ms\n",
      "Speed: 0.0ms preprocess, 182.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 42frame_90.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 168.0ms\n",
      "Speed: 6.2ms preprocess, 168.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 419.7ms\n",
      "Speed: 3.1ms preprocess, 419.7ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 156.7ms\n",
      "Speed: 11.5ms preprocess, 156.7ms inference, 11.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 310.6ms\n",
      "Speed: 0.0ms preprocess, 310.6ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 175.2ms\n",
      "Speed: 0.0ms preprocess, 175.2ms inference, 9.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 189.9ms\n",
      "Speed: 0.0ms preprocess, 189.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_240.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 142.9ms\n",
      "Speed: 6.6ms preprocess, 142.9ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_270.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 257.0ms\n",
      "Speed: 2.3ms preprocess, 257.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 195.1ms\n",
      "Speed: 3.7ms preprocess, 195.1ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_300.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 196.4ms\n",
      "Speed: 0.0ms preprocess, 196.4ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_330.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 204.1ms\n",
      "Speed: 5.3ms preprocess, 204.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_360.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 235.8ms\n",
      "Speed: 1.1ms preprocess, 235.8ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_390.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 407.0ms\n",
      "Speed: 6.6ms preprocess, 407.0ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 392.2ms\n",
      "Speed: 6.1ms preprocess, 392.2ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 52frame_90.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 402.7ms\n",
      "Speed: 5.0ms preprocess, 402.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 266.5ms\n",
      "Speed: 8.0ms preprocess, 266.5ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 252.8ms\n",
      "Speed: 6.5ms preprocess, 252.8ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 231.6ms\n",
      "Speed: 5.0ms preprocess, 231.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 246.8ms\n",
      "Speed: 5.0ms preprocess, 246.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 232.6ms\n",
      "Speed: 6.0ms preprocess, 232.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_240.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 240.4ms\n",
      "Speed: 5.0ms preprocess, 240.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_270.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 236.7ms\n",
      "Speed: 4.3ms preprocess, 236.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 227.9ms\n",
      "Speed: 4.0ms preprocess, 227.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_300.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 251.6ms\n",
      "Speed: 5.0ms preprocess, 251.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_330.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 409.4ms\n",
      "Speed: 4.0ms preprocess, 409.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_360.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 243.0ms\n",
      "Speed: 4.0ms preprocess, 243.0ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_390.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 243.1ms\n",
      "Speed: 0.0ms preprocess, 243.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_420.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 191.1ms\n",
      "Speed: 2.2ms preprocess, 191.1ms inference, 15.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_450.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 210.7ms\n",
      "Speed: 0.0ms preprocess, 210.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 235.5ms\n",
      "Speed: 5.4ms preprocess, 235.5ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 6frame_90.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 213.4ms\n",
      "Speed: 0.0ms preprocess, 213.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 195.6ms\n",
      "Speed: 8.0ms preprocess, 195.6ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 222.0ms\n",
      "Speed: 8.0ms preprocess, 222.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 275.8ms\n",
      "Speed: 8.0ms preprocess, 275.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 211.0ms\n",
      "Speed: 11.8ms preprocess, 211.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 221.3ms\n",
      "Speed: 13.6ms preprocess, 221.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_240.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 232.6ms\n",
      "Speed: 3.2ms preprocess, 232.6ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_270.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 220.1ms\n",
      "Speed: 0.0ms preprocess, 220.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 209.7ms\n",
      "Speed: 9.5ms preprocess, 209.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_300.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 207.4ms\n",
      "Speed: 6.9ms preprocess, 207.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_330.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 215.6ms\n",
      "Speed: 0.0ms preprocess, 215.6ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_360.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 221.5ms\n",
      "Speed: 15.6ms preprocess, 221.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_390.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 204.6ms\n",
      "Speed: 0.0ms preprocess, 204.6ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_420.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 236.0ms\n",
      "Speed: 9.5ms preprocess, 236.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_450.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 220.8ms\n",
      "Speed: 0.0ms preprocess, 220.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_480.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 203.2ms\n",
      "Speed: 16.4ms preprocess, 203.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_510.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 220.9ms\n",
      "Speed: 0.0ms preprocess, 220.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 208.2ms\n",
      "Speed: 7.6ms preprocess, 208.2ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 7frame_90.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 235.2ms\n",
      "Speed: 11.8ms preprocess, 235.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_0.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 268.7ms\n",
      "Speed: 0.0ms preprocess, 268.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_120.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 390.0ms\n",
      "Speed: 0.0ms preprocess, 390.0ms inference, 14.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_150.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 236.0ms\n",
      "Speed: 15.6ms preprocess, 236.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_180.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 278.8ms\n",
      "Speed: 3.6ms preprocess, 278.8ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_210.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 456.3ms\n",
      "Speed: 0.0ms preprocess, 456.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_30.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 286.2ms\n",
      "Speed: 0.0ms preprocess, 286.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_60.jpg, saved txt and image.\n",
      "\n",
      "0: 384x640 1 person, 294.0ms\n",
      "Speed: 0.0ms preprocess, 294.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed 8frame_90.jpg, saved txt and image.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load YOLOv8 model from Ultralytics\n",
    "yolo_model = YOLO('yolov8n.pt')  # Replace with your YOLOv8 model\n",
    "\n",
    "# Function to normalize landmark points\n",
    "def normalize_landmarks(landmarks, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    normalized = []\n",
    "    for lm in landmarks:\n",
    "        x = lm.x * w\n",
    "        y = lm.y * h\n",
    "        # Ensure the coordinates are within [0, 1]\n",
    "        normalized_x = np.clip(x / w, 0, 1)\n",
    "        normalized_y = np.clip(y / h, 0, 1)\n",
    "        normalized.append((normalized_x, normalized_y, lm.z, lm.visibility))\n",
    "    return normalized\n",
    "\n",
    "# Function to write YOLO format\n",
    "def write_yolo_format(yolo_detections, landmarks, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        # YOLO detection line\n",
    "        for det in yolo_detections:\n",
    "            # Normalize coordinates (ensure all within bounds [0, 1])\n",
    "            x_center = np.clip(det[0], 0, 1)\n",
    "            y_center = np.clip(det[1], 0, 1)\n",
    "            width = np.clip(det[2], 0, 1)\n",
    "            height = np.clip(det[3], 0, 1)\n",
    "            f.write(f\"0 {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "        # Pose landmarks line (assuming all keypoints belong to person class 2)\n",
    "        for i, lm in enumerate(landmarks):\n",
    "            f.write(f\"2 {lm[0]} {lm[1]} {lm[2]} {lm[3]}\\n\")\n",
    "\n",
    "# Main function to process images\n",
    "def process_images(input_folder, output_txt_folder, output_img_folder):\n",
    "    # Ensure output folders exist\n",
    "    os.makedirs(output_txt_folder, exist_ok=True)\n",
    "    os.makedirs(output_img_folder, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all files in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(('.jpg', '.png', '.jpeg')):  # Filter for image files\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            output_txt_path = os.path.join(output_txt_folder, f\"{os.path.splitext(file_name)[0]}.txt\")\n",
    "            output_img_path = os.path.join(output_img_folder, file_name)\n",
    "            \n",
    "            # Load input image\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Pose detection using MediaPipe\n",
    "            results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Initialize pose landmarks\n",
    "            normalized_landmarks = []\n",
    "            if results.pose_landmarks:\n",
    "                normalized_landmarks = normalize_landmarks(results.pose_landmarks.landmark, image.shape)\n",
    "            \n",
    "            # Run YOLOv8 for person detection\n",
    "            yolo_results = yolo_model(image)\n",
    "            \n",
    "            # Collect bounding box and confidence for person detection\n",
    "            yolo_detections = []\n",
    "            for result in yolo_results:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    # Only consider the 'person' class (usually class_id 0)\n",
    "                    if int(box.cls) == 0:\n",
    "                        # Extract and normalize bounding box coordinates\n",
    "                        x_center, y_center, width, height = box.xywh[0] / np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "                        yolo_detections.append((x_center, y_center, width, height))\n",
    "            \n",
    "            # Write the data to YOLO format .txt file\n",
    "            write_yolo_format(yolo_detections, normalized_landmarks, output_txt_path)\n",
    "            \n",
    "            # Save the output image with YOLO detections (optional: draw bounding boxes)\n",
    "            for box in yolo_results[0].boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get box coordinates\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "\n",
    "            # Save the image with detections\n",
    "            cv2.imwrite(output_img_path, image)\n",
    "            print(f\"Processed {file_name}, saved txt and image.\")\n",
    "\n",
    "# Specify folders\n",
    "input_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img'\n",
    "output_txt_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels'\n",
    "output_img_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl'\n",
    "\n",
    "# Run the processing function\n",
    "process_images(input_folder, output_txt_folder, output_img_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 284.3ms\n",
      "Speed: 2.3ms preprocess, 284.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 152.7ms\n",
      "Speed: 4.0ms preprocess, 152.7ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 152.5ms\n",
      "Speed: 8.0ms preprocess, 152.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 152.2ms\n",
      "Speed: 0.0ms preprocess, 152.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 135.9ms\n",
      "Speed: 14.0ms preprocess, 135.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 164.9ms\n",
      "Speed: 0.0ms preprocess, 164.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 172.2ms\n",
      "Speed: 5.3ms preprocess, 172.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 166.3ms\n",
      "Speed: 3.1ms preprocess, 166.3ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 147.8ms\n",
      "Speed: 4.5ms preprocess, 147.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 135.4ms\n",
      "Speed: 8.3ms preprocess, 135.4ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 154.7ms\n",
      "Speed: 0.0ms preprocess, 154.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 136.2ms\n",
      "Speed: 0.0ms preprocess, 136.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 163.5ms\n",
      "Speed: 0.0ms preprocess, 163.5ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_420.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_420.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_420.jpg\n",
      "\n",
      "0: 384x640 1 person, 147.2ms\n",
      "Speed: 0.0ms preprocess, 147.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_450.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_450.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_450.jpg\n",
      "\n",
      "0: 384x640 1 person, 292.0ms\n",
      "Speed: 0.0ms preprocess, 292.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_480.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_480.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_480.jpg\n",
      "\n",
      "0: 384x640 1 person, 151.5ms\n",
      "Speed: 6.3ms preprocess, 151.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_510.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_510.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_510.jpg\n",
      "\n",
      "0: 384x640 1 person, 184.1ms\n",
      "Speed: 1.0ms preprocess, 184.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_540.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_540.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_540.jpg\n",
      "\n",
      "0: 384x640 1 person, 190.4ms\n",
      "Speed: 2.7ms preprocess, 190.4ms inference, 9.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_570.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_570.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_570.jpg\n",
      "\n",
      "0: 384x640 1 person, 213.2ms\n",
      "Speed: 5.7ms preprocess, 213.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 175.3ms\n",
      "Speed: 7.9ms preprocess, 175.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 190.4ms\n",
      "Speed: 3.7ms preprocess, 190.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 215.1ms\n",
      "Speed: 0.0ms preprocess, 215.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 159.9ms\n",
      "Speed: 4.0ms preprocess, 159.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 183.4ms\n",
      "Speed: 0.0ms preprocess, 183.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 155.4ms\n",
      "Speed: 3.7ms preprocess, 155.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 164.3ms\n",
      "Speed: 15.6ms preprocess, 164.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 281.5ms\n",
      "Speed: 5.5ms preprocess, 281.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 214.4ms\n",
      "Speed: 3.3ms preprocess, 214.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 174.9ms\n",
      "Speed: 0.0ms preprocess, 174.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 210.4ms\n",
      "Speed: 3.0ms preprocess, 210.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 139.4ms\n",
      "Speed: 6.0ms preprocess, 139.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 185.0ms\n",
      "Speed: 2.4ms preprocess, 185.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 143.6ms\n",
      "Speed: 1.8ms preprocess, 143.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 180.4ms\n",
      "Speed: 8.0ms preprocess, 180.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 177.6ms\n",
      "Speed: 0.0ms preprocess, 177.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 168.6ms\n",
      "Speed: 0.0ms preprocess, 168.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 143.4ms\n",
      "Speed: 4.5ms preprocess, 143.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 162.9ms\n",
      "Speed: 11.5ms preprocess, 162.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 152.6ms\n",
      "Speed: 0.0ms preprocess, 152.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 140.0ms\n",
      "Speed: 8.0ms preprocess, 140.0ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 162.5ms\n",
      "Speed: 0.0ms preprocess, 162.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 371.9ms\n",
      "Speed: 3.8ms preprocess, 371.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 281.0ms\n",
      "Speed: 6.8ms preprocess, 281.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 316.7ms\n",
      "Speed: 0.0ms preprocess, 316.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 339.0ms\n",
      "Speed: 5.0ms preprocess, 339.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 184.3ms\n",
      "Speed: 0.0ms preprocess, 184.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 198.9ms\n",
      "Speed: 0.0ms preprocess, 198.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 226.2ms\n",
      "Speed: 10.1ms preprocess, 226.2ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 173.8ms\n",
      "Speed: 1.1ms preprocess, 173.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 200.8ms\n",
      "Speed: 11.1ms preprocess, 200.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 195.6ms\n",
      "Speed: 7.4ms preprocess, 195.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 204.3ms\n",
      "Speed: 5.3ms preprocess, 204.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 248.5ms\n",
      "Speed: 0.0ms preprocess, 248.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 228.8ms\n",
      "Speed: 0.0ms preprocess, 228.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 206.0ms\n",
      "Speed: 0.0ms preprocess, 206.0ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 236.9ms\n",
      "Speed: 0.0ms preprocess, 236.9ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 217.8ms\n",
      "Speed: 3.2ms preprocess, 217.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 228.2ms\n",
      "Speed: 0.0ms preprocess, 228.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 206.5ms\n",
      "Speed: 12.4ms preprocess, 206.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 190.6ms\n",
      "Speed: 3.1ms preprocess, 190.6ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 198.0ms\n",
      "Speed: 0.0ms preprocess, 198.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 212.0ms\n",
      "Speed: 0.0ms preprocess, 212.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 200.3ms\n",
      "Speed: 4.1ms preprocess, 200.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 189.7ms\n",
      "Speed: 0.0ms preprocess, 189.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 205.6ms\n",
      "Speed: 0.0ms preprocess, 205.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 193.8ms\n",
      "Speed: 0.0ms preprocess, 193.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 186.3ms\n",
      "Speed: 0.0ms preprocess, 186.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 179.6ms\n",
      "Speed: 2.6ms preprocess, 179.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 188.1ms\n",
      "Speed: 0.0ms preprocess, 188.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 193.7ms\n",
      "Speed: 1.2ms preprocess, 193.7ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 238.8ms\n",
      "Speed: 3.7ms preprocess, 238.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 220.7ms\n",
      "Speed: 0.0ms preprocess, 220.7ms inference, 16.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 190.6ms\n",
      "Speed: 0.0ms preprocess, 190.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 171.2ms\n",
      "Speed: 0.0ms preprocess, 171.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 207.8ms\n",
      "Speed: 1.1ms preprocess, 207.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_420.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_420.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_420.jpg\n",
      "\n",
      "0: 384x640 1 person, 205.1ms\n",
      "Speed: 0.0ms preprocess, 205.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_450.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_450.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_450.jpg\n",
      "\n",
      "0: 384x640 1 person, 204.3ms\n",
      "Speed: 0.0ms preprocess, 204.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 187.7ms\n",
      "Speed: 0.0ms preprocess, 187.7ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 209.4ms\n",
      "Speed: 0.0ms preprocess, 209.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 208.9ms\n",
      "Speed: 6.2ms preprocess, 208.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 269.4ms\n",
      "Speed: 3.0ms preprocess, 269.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 239.4ms\n",
      "Speed: 3.9ms preprocess, 239.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 205.7ms\n",
      "Speed: 8.5ms preprocess, 205.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 551.7ms\n",
      "Speed: 25.3ms preprocess, 551.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 242.2ms\n",
      "Speed: 4.6ms preprocess, 242.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 274.3ms\n",
      "Speed: 3.0ms preprocess, 274.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 365.5ms\n",
      "Speed: 0.0ms preprocess, 365.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 193.0ms\n",
      "Speed: 6.1ms preprocess, 193.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 206.7ms\n",
      "Speed: 0.0ms preprocess, 206.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 213.1ms\n",
      "Speed: 10.2ms preprocess, 213.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 201.2ms\n",
      "Speed: 11.9ms preprocess, 201.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_420.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_420.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_420.jpg\n",
      "\n",
      "0: 384x640 1 person, 233.1ms\n",
      "Speed: 10.0ms preprocess, 233.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_450.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_450.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_450.jpg\n",
      "\n",
      "0: 384x640 1 person, 225.5ms\n",
      "Speed: 5.0ms preprocess, 225.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_480.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_480.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_480.jpg\n",
      "\n",
      "0: 384x640 1 person, 252.1ms\n",
      "Speed: 0.0ms preprocess, 252.1ms inference, 9.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_510.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_510.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_510.jpg\n",
      "\n",
      "0: 384x640 1 person, 279.3ms\n",
      "Speed: 6.0ms preprocess, 279.3ms inference, 10.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 314.7ms\n",
      "Speed: 4.4ms preprocess, 314.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 236.3ms\n",
      "Speed: 0.0ms preprocess, 236.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 246.0ms\n",
      "Speed: 4.0ms preprocess, 246.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 228.8ms\n",
      "Speed: 3.5ms preprocess, 228.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 210.1ms\n",
      "Speed: 5.8ms preprocess, 210.1ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 235.6ms\n",
      "Speed: 3.0ms preprocess, 235.6ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 263.4ms\n",
      "Speed: 0.0ms preprocess, 263.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 250.9ms\n",
      "Speed: 3.3ms preprocess, 250.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 239.5ms\n",
      "Speed: 7.0ms preprocess, 239.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_90.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load YOLOv8 model from Ultralytics\n",
    "yolo_model = YOLO('yolov8n.pt')  # Replace with your YOLOv8 model\n",
    "\n",
    "# Function to normalize landmark points\n",
    "def normalize_landmarks(landmarks, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    normalized = []\n",
    "    for lm in landmarks:\n",
    "        x = lm.x * w\n",
    "        y = lm.y * h\n",
    "        # Ensure the coordinates are within [0, 1]\n",
    "        normalized_x = np.clip(x / w, 0, 1)\n",
    "        normalized_y = np.clip(y / h, 0, 1)\n",
    "        normalized.append((normalized_x, normalized_y, lm.z, lm.visibility))\n",
    "    return normalized\n",
    "\n",
    "# Function to write YOLO format\n",
    "def write_yolo_format(yolo_detections, landmarks, output_file, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    with open(output_file, 'w') as f:\n",
    "        # YOLO detection line for each detected object (person in this case)\n",
    "        for det in yolo_detections:\n",
    "            x_center, y_center, width, height = det\n",
    "            f.write(f\"0 {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "        # Pose landmarks line (assuming all keypoints belong to person class 2)\n",
    "        for lm in landmarks:\n",
    "            x, y, z, visibility = lm\n",
    "            f.write(f\"2 {x} {y}\\n\")\n",
    "\n",
    "# Function to process an image and save the results\n",
    "def process_image(image_path, txt_output_dir, img_output_dir):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Pose detection using MediaPipe\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Initialize an empty list for normalized landmarks\n",
    "    normalized_landmarks = []\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        # Normalize the pose landmarks for YOLO format\n",
    "        normalized_landmarks = normalize_landmarks(results.pose_landmarks.landmark, image.shape)\n",
    "\n",
    "    # Run YOLOv8 for person detection\n",
    "    yolo_results = yolo_model(image)\n",
    "\n",
    "    # Collect bounding box and confidence for person detection\n",
    "    yolo_detections = []\n",
    "    for result in yolo_results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Only consider the 'person' class (usually class_id 0)\n",
    "            if int(box.cls) == 0:\n",
    "                # Extract and normalize bounding box coordinates\n",
    "                x_center, y_center, width, height = box.xywh[0] / np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "                yolo_detections.append((x_center, y_center, width, height))\n",
    "    \n",
    "    # Define output file names\n",
    "    base_name = os.path.basename(image_path).split('.')[0]\n",
    "    txt_output_path = os.path.join(txt_output_dir, f\"{base_name}.txt\")\n",
    "    img_output_path = os.path.join(img_output_dir, f\"{base_name}.jpg\")\n",
    "    \n",
    "    # Write the data to YOLO format .txt file\n",
    "    write_yolo_format(yolo_detections, normalized_landmarks, txt_output_path, image.shape)\n",
    "    \n",
    "    # Save the detected image to output folder\n",
    "    cv2.imwrite(img_output_path, image)\n",
    "\n",
    "    print(f\"Processed {image_path}, saved .txt to {txt_output_path} and image to {img_output_path}\")\n",
    "\n",
    "# Main function to process a folder of images\n",
    "def process_folder(input_folder, txt_output_folder, img_output_folder):\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(txt_output_folder, exist_ok=True)\n",
    "    os.makedirs(img_output_folder, exist_ok=True)\n",
    "\n",
    "    # Loop through all images in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(('.jpg', '.jpeg', '.png')):  # Filter for image files\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            process_image(image_path, txt_output_folder, img_output_folder)\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img'\n",
    "txt_output_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels'\n",
    "img_output_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl'\n",
    "\n",
    "# Process the folder of images\n",
    "process_folder(input_folder, txt_output_folder, img_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 266.2ms\n",
      "Speed: 0.0ms preprocess, 266.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 170.0ms\n",
      "Speed: 4.0ms preprocess, 170.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 144.0ms\n",
      "Speed: 9.9ms preprocess, 144.0ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 163.1ms\n",
      "Speed: 9.8ms preprocess, 163.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 160.0ms\n",
      "Speed: 3.5ms preprocess, 160.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 206.5ms\n",
      "Speed: 2.7ms preprocess, 206.5ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 222.7ms\n",
      "Speed: 6.5ms preprocess, 222.7ms inference, 14.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 171.2ms\n",
      "Speed: 0.0ms preprocess, 171.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 161.5ms\n",
      "Speed: 3.5ms preprocess, 161.5ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 189.8ms\n",
      "Speed: 0.0ms preprocess, 189.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 195.9ms\n",
      "Speed: 5.3ms preprocess, 195.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 191.2ms\n",
      "Speed: 5.4ms preprocess, 191.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 278.4ms\n",
      "Speed: 9.5ms preprocess, 278.4ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_420.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_420.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_420.jpg\n",
      "\n",
      "0: 384x640 1 person, 241.7ms\n",
      "Speed: 2.6ms preprocess, 241.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_450.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_450.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_450.jpg\n",
      "\n",
      "0: 384x640 1 person, 177.5ms\n",
      "Speed: 5.2ms preprocess, 177.5ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_480.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_480.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_480.jpg\n",
      "\n",
      "0: 384x640 1 person, 206.2ms\n",
      "Speed: 2.6ms preprocess, 206.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_510.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_510.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_510.jpg\n",
      "\n",
      "0: 384x640 1 person, 181.3ms\n",
      "Speed: 5.8ms preprocess, 181.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_540.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_540.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_540.jpg\n",
      "\n",
      "0: 384x640 1 person, 155.1ms\n",
      "Speed: 3.4ms preprocess, 155.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_570.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_570.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_570.jpg\n",
      "\n",
      "0: 384x640 1 person, 137.0ms\n",
      "Speed: 3.5ms preprocess, 137.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 188.6ms\n",
      "Speed: 0.0ms preprocess, 188.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\1frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\1frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\1frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 135.4ms\n",
      "Speed: 1.6ms preprocess, 135.4ms inference, 9.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 189.7ms\n",
      "Speed: 3.5ms preprocess, 189.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 186.4ms\n",
      "Speed: 5.3ms preprocess, 186.4ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 223.0ms\n",
      "Speed: 0.0ms preprocess, 223.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 198.1ms\n",
      "Speed: 6.1ms preprocess, 198.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 220.1ms\n",
      "Speed: 4.4ms preprocess, 220.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 189.2ms\n",
      "Speed: 5.1ms preprocess, 189.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 174.6ms\n",
      "Speed: 4.8ms preprocess, 174.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\2frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\2frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\2frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 201.7ms\n",
      "Speed: 12.0ms preprocess, 201.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 202.5ms\n",
      "Speed: 4.5ms preprocess, 202.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 179.6ms\n",
      "Speed: 4.8ms preprocess, 179.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 200.6ms\n",
      "Speed: 0.0ms preprocess, 200.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 216.6ms\n",
      "Speed: 5.5ms preprocess, 216.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 262.7ms\n",
      "Speed: 14.8ms preprocess, 262.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 397.6ms\n",
      "Speed: 0.0ms preprocess, 397.6ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 199.1ms\n",
      "Speed: 9.8ms preprocess, 199.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 197.6ms\n",
      "Speed: 5.0ms preprocess, 197.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\32frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\32frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\32frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 298.7ms\n",
      "Speed: 10.7ms preprocess, 298.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 271.1ms\n",
      "Speed: 6.0ms preprocess, 271.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 302.1ms\n",
      "Speed: 11.3ms preprocess, 302.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 180.5ms\n",
      "Speed: 8.5ms preprocess, 180.5ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 205.4ms\n",
      "Speed: 0.0ms preprocess, 205.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 171.4ms\n",
      "Speed: 0.0ms preprocess, 171.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 172.7ms\n",
      "Speed: 9.8ms preprocess, 172.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 168.7ms\n",
      "Speed: 7.0ms preprocess, 168.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 191.8ms\n",
      "Speed: 2.6ms preprocess, 191.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 169.0ms\n",
      "Speed: 6.2ms preprocess, 169.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 202.2ms\n",
      "Speed: 5.4ms preprocess, 202.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\42frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\42frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\42frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 187.9ms\n",
      "Speed: 7.7ms preprocess, 187.9ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 223.5ms\n",
      "Speed: 6.2ms preprocess, 223.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 260.0ms\n",
      "Speed: 0.0ms preprocess, 260.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 296.9ms\n",
      "Speed: 0.0ms preprocess, 296.9ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 278.9ms\n",
      "Speed: 0.0ms preprocess, 278.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 382.7ms\n",
      "Speed: 12.4ms preprocess, 382.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 496.4ms\n",
      "Speed: 11.1ms preprocess, 496.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 354.7ms\n",
      "Speed: 0.0ms preprocess, 354.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 377.3ms\n",
      "Speed: 12.5ms preprocess, 377.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 254.3ms\n",
      "Speed: 6.1ms preprocess, 254.3ms inference, 8.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 247.8ms\n",
      "Speed: 4.0ms preprocess, 247.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 276.3ms\n",
      "Speed: 6.2ms preprocess, 276.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 366.4ms\n",
      "Speed: 5.1ms preprocess, 366.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 256.4ms\n",
      "Speed: 3.6ms preprocess, 256.4ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\52frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\52frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\52frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 232.9ms\n",
      "Speed: 10.4ms preprocess, 232.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 235.4ms\n",
      "Speed: 11.1ms preprocess, 235.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 270.2ms\n",
      "Speed: 96.8ms preprocess, 270.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 277.8ms\n",
      "Speed: 4.8ms preprocess, 277.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 215.6ms\n",
      "Speed: 0.0ms preprocess, 215.6ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 220.5ms\n",
      "Speed: 8.7ms preprocess, 220.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 250.0ms\n",
      "Speed: 8.9ms preprocess, 250.0ms inference, 11.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 236.5ms\n",
      "Speed: 5.6ms preprocess, 236.5ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 329.3ms\n",
      "Speed: 2.4ms preprocess, 329.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 261.5ms\n",
      "Speed: 8.1ms preprocess, 261.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 249.3ms\n",
      "Speed: 4.7ms preprocess, 249.3ms inference, 11.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 271.3ms\n",
      "Speed: 7.5ms preprocess, 271.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 604.2ms\n",
      "Speed: 153.9ms preprocess, 604.2ms inference, 10.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_420.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_420.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_420.jpg\n",
      "\n",
      "0: 384x640 1 person, 225.6ms\n",
      "Speed: 7.9ms preprocess, 225.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_450.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_450.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_450.jpg\n",
      "\n",
      "0: 384x640 1 person, 377.2ms\n",
      "Speed: 31.4ms preprocess, 377.2ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 702.9ms\n",
      "Speed: 5.2ms preprocess, 702.9ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\6frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\6frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\6frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 302.3ms\n",
      "Speed: 4.6ms preprocess, 302.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 277.4ms\n",
      "Speed: 3.6ms preprocess, 277.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 262.2ms\n",
      "Speed: 0.0ms preprocess, 262.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 204.4ms\n",
      "Speed: 6.5ms preprocess, 204.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 283.4ms\n",
      "Speed: 8.0ms preprocess, 283.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 226.9ms\n",
      "Speed: 5.7ms preprocess, 226.9ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_240.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_240.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_240.jpg\n",
      "\n",
      "0: 384x640 1 person, 261.5ms\n",
      "Speed: 0.0ms preprocess, 261.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_270.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_270.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_270.jpg\n",
      "\n",
      "0: 384x640 1 person, 238.4ms\n",
      "Speed: 6.4ms preprocess, 238.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 214.4ms\n",
      "Speed: 0.0ms preprocess, 214.4ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_300.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_300.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_300.jpg\n",
      "\n",
      "0: 384x640 1 person, 198.8ms\n",
      "Speed: 0.0ms preprocess, 198.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_330.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_330.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_330.jpg\n",
      "\n",
      "0: 384x640 1 person, 239.7ms\n",
      "Speed: 4.1ms preprocess, 239.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_360.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_360.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_360.jpg\n",
      "\n",
      "0: 384x640 1 person, 219.6ms\n",
      "Speed: 9.0ms preprocess, 219.6ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_390.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_390.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_390.jpg\n",
      "\n",
      "0: 384x640 1 person, 220.5ms\n",
      "Speed: 0.0ms preprocess, 220.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_420.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_420.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_420.jpg\n",
      "\n",
      "0: 384x640 1 person, 256.7ms\n",
      "Speed: 3.7ms preprocess, 256.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_450.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_450.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_450.jpg\n",
      "\n",
      "0: 384x640 1 person, 253.0ms\n",
      "Speed: 6.3ms preprocess, 253.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_480.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_480.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_480.jpg\n",
      "\n",
      "0: 384x640 1 person, 204.4ms\n",
      "Speed: 6.5ms preprocess, 204.4ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_510.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_510.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_510.jpg\n",
      "\n",
      "0: 384x640 1 person, 250.6ms\n",
      "Speed: 0.0ms preprocess, 250.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 205.3ms\n",
      "Speed: 15.8ms preprocess, 205.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\7frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\7frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\7frame_90.jpg\n",
      "\n",
      "0: 384x640 1 person, 356.3ms\n",
      "Speed: 8.7ms preprocess, 356.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_0.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_0.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_0.jpg\n",
      "\n",
      "0: 384x640 1 person, 232.5ms\n",
      "Speed: 3.9ms preprocess, 232.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_120.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_120.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_120.jpg\n",
      "\n",
      "0: 384x640 1 person, 195.1ms\n",
      "Speed: 1.7ms preprocess, 195.1ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_150.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_150.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_150.jpg\n",
      "\n",
      "0: 384x640 1 person, 227.1ms\n",
      "Speed: 0.0ms preprocess, 227.1ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_180.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_180.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_180.jpg\n",
      "\n",
      "0: 384x640 1 person, 203.7ms\n",
      "Speed: 11.5ms preprocess, 203.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_210.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_210.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_210.jpg\n",
      "\n",
      "0: 384x640 1 person, 227.7ms\n",
      "Speed: 8.5ms preprocess, 227.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_30.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_30.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_30.jpg\n",
      "\n",
      "0: 384x640 1 person, 200.3ms\n",
      "Speed: 4.8ms preprocess, 200.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_60.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_60.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_60.jpg\n",
      "\n",
      "0: 384x640 1 person, 209.1ms\n",
      "Speed: 0.0ms preprocess, 209.1ms inference, 12.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Processed C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img\\8frame_90.jpg, saved .txt to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels\\8frame_90.txt and image to C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl\\8frame_90.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load YOLOv8 model from Ultralytics\n",
    "yolo_model = YOLO('yolov8n.pt')  # Replace with your YOLOv8 model\n",
    "\n",
    "# Function to normalize landmark points\n",
    "def normalize_landmarks(landmarks, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    normalized = []\n",
    "    for lm in landmarks:\n",
    "        x = lm.x * w\n",
    "        y = lm.y * h\n",
    "        # Normalize coordinates to [0, 1]\n",
    "        normalized_x = np.clip(x / w, 0, 1)\n",
    "        normalized_y = np.clip(y / h, 0, 1)\n",
    "        # Include z and visibility, but YOLO format does not use them\n",
    "        normalized.append((normalized_x, normalized_y))\n",
    "    return normalized\n",
    "\n",
    "# Function to write YOLO format\n",
    "def write_yolo_format(yolo_detections, landmarks, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        # YOLO detection line for each detected object (person in this case)\n",
    "        for det in yolo_detections:\n",
    "            x_center, y_center, width, height = det\n",
    "            if all(0 <= val <= 1 for val in [x_center, y_center, width, height]):\n",
    "                f.write(f\"0 {x_center} {y_center} {width} {height} \")  # Space added for subsequent landmarks\n",
    "\n",
    "        # Pose landmarks line (appending the class '2' after each pair of coordinates)\n",
    "        for lm in landmarks:\n",
    "            x, y = lm\n",
    "            if 0 <= x <= 1 and 0 <= y <= 1:\n",
    "                f.write(f\"{x} {y} 2 \")  # '2' after each coordinate pair\n",
    "\n",
    "        f.write('\\n')  # Add a newline character at the end of the file\n",
    "\n",
    "\n",
    "# Function to process an image and save the results\n",
    "def process_image(image_path, txt_output_dir, img_output_dir):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Pose detection using MediaPipe\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Initialize an empty list for normalized landmarks\n",
    "    normalized_landmarks = []\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        # Normalize the pose landmarks for YOLO format\n",
    "        normalized_landmarks = normalize_landmarks(results.pose_landmarks.landmark, image.shape)\n",
    "\n",
    "    # Run YOLOv8 for person detection\n",
    "    yolo_results = yolo_model(image)\n",
    "\n",
    "    # Collect bounding box and confidence for person detection\n",
    "    yolo_detections = []\n",
    "    for result in yolo_results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Only consider the 'person' class (usually class_id 0)\n",
    "            if int(box.cls) == 0:\n",
    "                # Extract and normalize bounding box coordinates\n",
    "                x_center, y_center, width, height = box.xywh[0] / np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "                yolo_detections.append((x_center, y_center, width, height))\n",
    "    \n",
    "    # Define output file names\n",
    "    base_name = os.path.basename(image_path).split('.')[0]\n",
    "    txt_output_path = os.path.join(txt_output_dir, f\"{base_name}.txt\")\n",
    "    img_output_path = os.path.join(img_output_dir, f\"{base_name}.jpg\")\n",
    "    \n",
    "    # Write the data to YOLO format .txt file\n",
    "    write_yolo_format(yolo_detections, normalized_landmarks, txt_output_path)\n",
    "    \n",
    "    # Save the detected image to output folder\n",
    "    cv2.imwrite(img_output_path, image)\n",
    "\n",
    "    print(f\"Processed {image_path}, saved .txt to {txt_output_path} and image to {img_output_path}\")\n",
    "\n",
    "# Main function to process a folder of images\n",
    "def process_folder(input_folder, txt_output_folder, img_output_folder):\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(txt_output_folder, exist_ok=True)\n",
    "    os.makedirs(img_output_folder, exist_ok=True)\n",
    "\n",
    "    # Loop through all images in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(('.jpg', '.jpeg', '.png')):  # Filter for image files\n",
    "            image_path = os.path.join(input_folder, file_name)\n",
    "            process_image(image_path, txt_output_folder, img_output_folder)\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img'\n",
    "txt_output_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels'\n",
    "img_output_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\img_lbl'\n",
    "\n",
    "# Process the folder of images\n",
    "process_folder(input_folder, txt_output_folder, img_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
