{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 253.8ms\n",
      "Speed: 15.6ms preprocess, 253.8ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x512 1 person, 289.3ms\n",
      "Speed: 5.0ms preprocess, 289.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 480x640 (no detections), 251.7ms\n",
      "Speed: 0.0ms preprocess, 251.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x544 1 person, 368.7ms\n",
      "Speed: 0.0ms preprocess, 368.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "flip_idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize YOLO and Mediapipe pose models\n",
    "yolo_model = YOLO('yolov8n.pt')  # YOLOv8n for lightweight detection\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Folder paths\n",
    "image_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\images' \n",
    "output_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels' \n",
    "\n",
    "# Ensure output folder exists\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# List to store all flip_idx (landmarks)\n",
    "all_flip_idx = []\n",
    "\n",
    "# Initialize Mediapipe pose estimation model\n",
    "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1) as pose:\n",
    "    # Iterate through all images in the folder\n",
    "    for image_name in os.listdir(image_folder):\n",
    "        if image_name.endswith('.jpg') or image_name.endswith('.png'): \n",
    "            # Load the image\n",
    "            image_path = os.path.join(image_folder, image_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Use YOLO to detect persons in the image\n",
    "            yolo_results = yolo_model(image)\n",
    "            detections = yolo_results[0]  # YOLO detections for the image\n",
    "            \n",
    "            # Prepare to store detection results\n",
    "            txt_lines = []\n",
    "            \n",
    "            # Process each detected person\n",
    "            for det in detections.boxes:\n",
    "                if det.cls == 0:  # '0' class for person detection\n",
    "                    # Get YOLO bounding box in YOLO format\n",
    "                    x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())\n",
    "                    box_w = (x2 - x1) / image.shape[1]\n",
    "                    box_h = (y2 - y1) / image.shape[0]\n",
    "                    box_x_center = (x1 + x2) / 2 / image.shape[1]\n",
    "                    box_y_center = (y1 + y2) / 2 / image.shape[0]\n",
    "                    \n",
    "                    # Crop detected person for pose estimation\n",
    "                    person_crop = image[y1:y2, x1:x2]\n",
    "                    person_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Perform pose estimation with Mediapipe\n",
    "                    results = pose.process(person_rgb)\n",
    "                    \n",
    "                    if results.pose_landmarks:\n",
    "                        # Prepare keypoints for the .txt file\n",
    "                        keypoints = []\n",
    "                        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "                            keypoints.append(f'{landmark.x} {landmark.y} {landmark.visibility}')\n",
    "                            all_flip_idx.append(idx)  # Add to flip_idx list\n",
    "\n",
    "                        # Combine bounding box and keypoints into a single line\n",
    "                        txt_line = f'0 {box_x_center} {box_y_center} {box_w} {box_h} ' + ' '.join(keypoints)\n",
    "                        txt_lines.append(txt_line)\n",
    "            \n",
    "            # Write results to a .txt file with the same name as the image\n",
    "            output_txt_path = os.path.join(output_folder, os.path.splitext(image_name)[0] + '.txt')\n",
    "            with open(output_txt_path, 'w') as f:\n",
    "                f.write('\\n'.join(txt_lines))\n",
    "\n",
    "# Remove duplicates from flip_idx and return the list\n",
    "flip_idx = list(set(all_flip_idx))\n",
    "print(\"flip_idx:\", flip_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 202.4ms\n",
      "Speed: 2.6ms preprocess, 202.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x512 1 person, 247.7ms\n",
      "Speed: 0.0ms preprocess, 247.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 480x640 (no detections), 194.0ms\n",
      "Speed: 0.0ms preprocess, 194.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x544 1 person, 188.1ms\n",
      "Speed: 0.0ms preprocess, 188.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "flip_idx: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize YOLO and Mediapipe pose models\n",
    "yolo_model = YOLO('yolov8n.pt')  # YOLOv8n for lightweight detection\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Folder paths\n",
    "image_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\images' \n",
    "output_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\labels'\n",
    "annotated_folder = r'C:\\Users\\Admin\\Downloads\\Body Language Detection with mediapipe\\images1'\n",
    "\n",
    "# Ensure output and annotated image folders exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "if not os.path.exists(annotated_folder):\n",
    "    os.makedirs(annotated_folder)\n",
    "\n",
    "# List to store all flip_idx (landmarks)\n",
    "all_flip_idx = []\n",
    "\n",
    "# Initialize Mediapipe pose estimation model\n",
    "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1) as pose:\n",
    "    # Iterate through all images in the folder\n",
    "    for image_name in os.listdir(image_folder):\n",
    "        if image_name.endswith('.jpg') or image_name.endswith('.png'): \n",
    "            # Load the image\n",
    "            image_path = os.path.join(image_folder, image_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Use YOLO to detect persons in the image\n",
    "            yolo_results = yolo_model(image)\n",
    "            detections = yolo_results[0]  # YOLO detections for the image\n",
    "            \n",
    "            # Prepare to store detection results\n",
    "            txt_lines = []\n",
    "            \n",
    "            # Process each detected person\n",
    "            for det in detections.boxes:\n",
    "                if det.cls == 0:  # '0' class for person detection\n",
    "                    # Get YOLO bounding box in YOLO format\n",
    "                    x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())\n",
    "                    box_w = (x2 - x1) / image.shape[1]\n",
    "                    box_h = (y2 - y1) / image.shape[0]\n",
    "                    box_x_center = (x1 + x2) / 2 / image.shape[1]\n",
    "                    box_y_center = (y1 + y2) / 2 / image.shape[0]\n",
    "                    \n",
    "                    # Draw YOLO bounding box on the image\n",
    "                    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Crop detected person for pose estimation\n",
    "                    person_crop = image[y1:y2, x1:x2]\n",
    "                    person_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Perform pose estimation with Mediapipe\n",
    "                    results = pose.process(person_rgb)\n",
    "                    \n",
    "                    if results.pose_landmarks:\n",
    "                        # Draw pose landmarks on the cropped person and overlay it back on the original image\n",
    "                        mp_drawing.draw_landmarks(image[y1:y2, x1:x2], results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "                        # Prepare keypoints for the .txt file\n",
    "                        keypoints = []\n",
    "                        for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "                            keypoints.append(f'{landmark.x} {landmark.y} {landmark.visibility}')\n",
    "                            all_flip_idx.append(idx)  # Add to flip_idx list\n",
    "\n",
    "                        # Combine bounding box and keypoints into a single line\n",
    "                        txt_line = f'0 {box_x_center} {box_y_center} {box_w} {box_h} ' + ' '.join(keypoints)\n",
    "                        txt_lines.append(txt_line)\n",
    "            \n",
    "            # Write results to a .txt file with the same name as the image\n",
    "            output_txt_path = os.path.join(output_folder, os.path.splitext(image_name)[0] + '.txt')\n",
    "            with open(output_txt_path, 'w') as f:\n",
    "                f.write('\\n'.join(txt_lines))\n",
    "            \n",
    "            # Save the annotated image\n",
    "            annotated_image_path = os.path.join(annotated_folder, image_name)\n",
    "            cv2.imwrite(annotated_image_path, image)\n",
    "\n",
    "# Remove duplicates from flip_idx and return the list\n",
    "flip_idx = list(set(all_flip_idx))\n",
    "print(\"flip_idx:\", flip_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
